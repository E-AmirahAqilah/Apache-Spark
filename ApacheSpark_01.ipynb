{"cells":[{"cell_type":"markdown","metadata":{"id":"M6PRcw_7Y1V2"},"source":["<img src=\"images/cads-logo.png\" style=\"height: 100px;\" align=left> <img src=\"images/apache_spark.png\" style=\"height: 20%;width:20%\" align=right>"]},{"cell_type":"markdown","metadata":{"id":"FDEGYQ3sY1V6"},"source":["# Apache Spark DataFrames and Spark SQL"]},{"cell_type":"markdown","metadata":{"id":"4VIjUo6LY1V6"},"source":["Apache Spark is a platform for distributed data processing, and it is particularly well-suited for dealing with massive data sets. The data sets that they do not readily fit within the memory or capacity of a single server.\n","\n","Apache Spark has a modular architecture. A core platform is called Apache Spark core, and there are several modules, which run on top of the core platform.\n","\n","In this notebook, we will mostly learn about DataFrames and work with Spark SQL. Apache Spark supports multiple languages, including:\n","- Scala\n","- Python\n","- Java\n","- Python\n","- R\n","\n","Apache Spark's core data structure is the Resilient Distributed Dataset (RDD). RDD is a low-level object that lets Spark work by splitting data across multiple nodes in the cluster. However, working directly with RDDs is hard. Therefore, data scientists and data engineers prefer to use the Spark DataFrame abstraction built on top of RDDs.\n","\n","We are particularly interested in a data structure called DataFrames. DataFrames are a set of data that are organized into columns and rows. The columns have names, and the rows have a schema. Therefore, in this way, they are very similar or analogous to tables in relational databases. Not only DataFrames are easier to understand, but also they are more optimized for complicated operations than RDDs."]},{"cell_type":"markdown","metadata":{"id":"3eT6VEEQY1V8"},"source":["There are a couple of different ways of working with DataFrames. One way is to use the DataFrame API, and basically, that is structured around using methods on DataFrame objects. The second way is Spark SQL that allows us to enter SQL queries which are executed on DataFrames, and those DataFrames are registered as tables."]},{"cell_type":"markdown","metadata":{"id":"mkxBjiFWY1V8"},"source":["### Setup Apache Spark on Jupyter\n","To start working with DataFrames, first of all, we have to create a `SparkSession` object from `SparkContext`. The `SparkContext` is a connection to the running cluster, and `SparkSession` is an interface with that connection."]},{"cell_type":"code","execution_count":1,"metadata":{"id":"8UcR9lW9Y1V9","executionInfo":{"status":"ok","timestamp":1732774301992,"user_tz":-480,"elapsed":803,"user":{"displayName":"Nurul Izzati Darul Zaman","userId":"02130484108045505747"}}},"outputs":[],"source":["#!pip install pyspark"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"TdyAR9sOY1V_","executionInfo":{"status":"ok","timestamp":1732774303987,"user_tz":-480,"elapsed":1279,"user":{"displayName":"Nurul Izzati Darul Zaman","userId":"02130484108045505747"}}},"outputs":[],"source":["import pyspark"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"TRV1BZKtY1WA","executionInfo":{"status":"ok","timestamp":1732774303987,"user_tz":-480,"elapsed":2,"user":{"displayName":"Nurul Izzati Darul Zaman","userId":"02130484108045505747"}}},"outputs":[],"source":["from pyspark.sql import SparkSession"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"6zVPyVdQY1WA","executionInfo":{"status":"ok","timestamp":1732774313656,"user_tz":-480,"elapsed":9671,"user":{"displayName":"Nurul Izzati Darul Zaman","userId":"02130484108045505747"}}},"outputs":[],"source":["spark = SparkSession.builder.getOrCreate()"]},{"cell_type":"markdown","metadata":{"id":"mmwMqYtnY1WC"},"source":["The previous line of code returns an existing `SparkSession` if there's already one in the environment, or creates a new one if necessary."]},{"cell_type":"code","execution_count":5,"metadata":{"id":"U8UOS3OVY1WC","colab":{"base_uri":"https://localhost:8080/","height":219},"executionInfo":{"status":"ok","timestamp":1732774315426,"user_tz":-480,"elapsed":1781,"user":{"displayName":"Nurul Izzati Darul Zaman","userId":"02130484108045505747"}},"outputId":"2edfc260-164f-4345-b37d-a664c427c180"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<pyspark.sql.session.SparkSession at 0x7bbd27331060>"],"text/html":["\n","            <div>\n","                <p><b>SparkSession - in-memory</b></p>\n","                \n","        <div>\n","            <p><b>SparkContext</b></p>\n","\n","            <p><a href=\"http://56081ba22c8b:4040\">Spark UI</a></p>\n","\n","            <dl>\n","              <dt>Version</dt>\n","                <dd><code>v3.5.3</code></dd>\n","              <dt>Master</dt>\n","                <dd><code>local[*]</code></dd>\n","              <dt>AppName</dt>\n","                <dd><code>pyspark-shell</code></dd>\n","            </dl>\n","        </div>\n","        \n","            </div>\n","        "]},"metadata":{},"execution_count":5}],"source":["spark"]},{"cell_type":"markdown","metadata":{"id":"WLkUlxLKY1WD"},"source":["### Make the data set folder accessible\n","\n","In the following cells, we are going to load a file called `location_temp.csv`, which is a time-series file which contains loacations of sensors and the temperatures taken at particular periods of time."]},{"cell_type":"code","execution_count":6,"metadata":{"id":"Gby87hxFY1WD","executionInfo":{"status":"ok","timestamp":1732774315426,"user_tz":-480,"elapsed":2,"user":{"displayName":"Nurul Izzati Darul Zaman","userId":"02130484108045505747"}}},"outputs":[],"source":["import os\n","MAIN_DIRECTORY = os.getcwd()"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"so3vInfrY1WE","executionInfo":{"status":"ok","timestamp":1732774371580,"user_tz":-480,"elapsed":561,"user":{"displayName":"Nurul Izzati Darul Zaman","userId":"02130484108045505747"}}},"outputs":[],"source":["file_path =MAIN_DIRECTORY+\"/data/location_temp.csv\""]},{"cell_type":"markdown","metadata":{"id":"f5VDO432Y1WE"},"source":["## Get Started with Spark DataFrames\n","To create a Spark DataFrame by loading a csv file, we can use `spark.read` function as follows."]},{"cell_type":"code","execution_count":8,"metadata":{"id":"bW6agz07Y1WE","executionInfo":{"status":"ok","timestamp":1732774380732,"user_tz":-480,"elapsed":7478,"user":{"displayName":"Nurul Izzati Darul Zaman","userId":"02130484108045505747"}}},"outputs":[],"source":["df1 = spark.read.format(\"csv\").option(\"header\",\"true\").load(file_path)"]},{"cell_type":"markdown","metadata":{"id":"20OINYZZY1WF"},"source":["We can use `head(n)` method to show the heading of this data frame. `n` is the number of rows and its default value is 1."]},{"cell_type":"code","execution_count":9,"metadata":{"id":"lnoMfYg-Y1WF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1732774381069,"user_tz":-480,"elapsed":339,"user":{"displayName":"Nurul Izzati Darul Zaman","userId":"02130484108045505747"}},"outputId":"9159c0a9-bfd8-4bd5-fae6-2ae59e24210f"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[Row(event_date='03/04/2019 19:48:06', location_id='loc0', temp_celcius='29'),\n"," Row(event_date='03/04/2019 19:53:06', location_id='loc0', temp_celcius='27'),\n"," Row(event_date='03/04/2019 19:58:06', location_id='loc0', temp_celcius='28'),\n"," Row(event_date='03/04/2019 20:03:06', location_id='loc0', temp_celcius='30'),\n"," Row(event_date='03/04/2019 20:08:06', location_id='loc0', temp_celcius='27')]"]},"metadata":{},"execution_count":9}],"source":["df1.head(5)"]},{"cell_type":"markdown","metadata":{"id":"TN4UPWIbY1WG"},"source":["If we want to show the data in a tabular format, we can use `.show(n)` method. `n` is the number of rows and its default value is 20."]},{"cell_type":"code","execution_count":10,"metadata":{"id":"8yqhHC2lY1WG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1732774381387,"user_tz":-480,"elapsed":320,"user":{"displayName":"Nurul Izzati Darul Zaman","userId":"02130484108045505747"}},"outputId":"4be89844-7628-465b-dcdf-aa39b7dc71aa"},"outputs":[{"output_type":"stream","name":"stdout","text":["+-------------------+-----------+------------+\n","|         event_date|location_id|temp_celcius|\n","+-------------------+-----------+------------+\n","|03/04/2019 19:48:06|       loc0|          29|\n","|03/04/2019 19:53:06|       loc0|          27|\n","|03/04/2019 19:58:06|       loc0|          28|\n","|03/04/2019 20:03:06|       loc0|          30|\n","|03/04/2019 20:08:06|       loc0|          27|\n","|03/04/2019 20:13:06|       loc0|          27|\n","|03/04/2019 20:18:06|       loc0|          27|\n","|03/04/2019 20:23:06|       loc0|          29|\n","|03/04/2019 20:28:06|       loc0|          32|\n","|03/04/2019 20:33:06|       loc0|          35|\n","|03/04/2019 20:38:06|       loc0|          32|\n","|03/04/2019 20:43:06|       loc0|          28|\n","|03/04/2019 20:48:06|       loc0|          28|\n","|03/04/2019 20:53:06|       loc0|          32|\n","|03/04/2019 20:58:06|       loc0|          34|\n","|03/04/2019 21:03:06|       loc0|          33|\n","|03/04/2019 21:08:06|       loc0|          27|\n","|03/04/2019 21:13:06|       loc0|          28|\n","|03/04/2019 21:18:06|       loc0|          33|\n","|03/04/2019 21:23:06|       loc0|          28|\n","+-------------------+-----------+------------+\n","only showing top 20 rows\n","\n"]}],"source":["df1.show()"]},{"cell_type":"markdown","metadata":{"id":"DbGlPmQ2Y1WG"},"source":["To know the number of rows in the DataFrame, there is a useful method called `count()` that performs a count on the rows in a DataFrame."]},{"cell_type":"code","execution_count":11,"metadata":{"id":"9j6YsAhfY1WH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1732774388746,"user_tz":-480,"elapsed":2428,"user":{"displayName":"Nurul Izzati Darul Zaman","userId":"02130484108045505747"}},"outputId":"b6edd502-7820-44f6-a943-a68226e4382d"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["500000"]},"metadata":{},"execution_count":11}],"source":["df1.count()"]},{"cell_type":"markdown","metadata":{"id":"zFsRwQqJY1WH"},"source":["One of the useful methods in DataFrame API is `printSchema()` that prints out the schema in the tree format."]},{"cell_type":"code","execution_count":12,"metadata":{"id":"FZANtYxjY1WI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1732774388747,"user_tz":-480,"elapsed":3,"user":{"displayName":"Nurul Izzati Darul Zaman","userId":"02130484108045505747"}},"outputId":"121ddc12-27b3-4a46-e6ab-798897ec2e36"},"outputs":[{"output_type":"stream","name":"stdout","text":["root\n"," |-- event_date: string (nullable = true)\n"," |-- location_id: string (nullable = true)\n"," |-- temp_celcius: string (nullable = true)\n","\n"]}],"source":["df1.printSchema()"]},{"cell_type":"markdown","metadata":{"id":"2yNnNi16Y1WI"},"source":["### Rename column names\n","\n","Now, let's load another file. In the data folder, we have another file called `utilization.csv`. This file does not have a header row. If we want to use the csv file schema, Spark provides an option to infer the columns' data types automatically. The following cells show how we can work with this type of csv file."]},{"cell_type":"code","execution_count":13,"metadata":{"id":"22rVk6TVY1WI","executionInfo":{"status":"ok","timestamp":1732774390993,"user_tz":-480,"elapsed":400,"user":{"displayName":"Nurul Izzati Darul Zaman","userId":"02130484108045505747"}}},"outputs":[],"source":["file_path =MAIN_DIRECTORY+\"/data/utilization.csv\""]},{"cell_type":"code","execution_count":14,"metadata":{"id":"2FFWHnCcY1WJ","executionInfo":{"status":"ok","timestamp":1732774395283,"user_tz":-480,"elapsed":3740,"user":{"displayName":"Nurul Izzati Darul Zaman","userId":"02130484108045505747"}}},"outputs":[],"source":["df2 = spark.read.format(\"csv\").option(\"header\",\"false\").option(\"inferSchema\",\"true\").load(file_path)"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"MEvDMB5tY1WJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1732774396220,"user_tz":-480,"elapsed":946,"user":{"displayName":"Nurul Izzati Darul Zaman","userId":"02130484108045505747"}},"outputId":"7fe665a0-43e1-4f63-8ce5-a5e4f4386013"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["500000"]},"metadata":{},"execution_count":15}],"source":["df2.count()"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"BgFvhaclY1WK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1732774402356,"user_tz":-480,"elapsed":333,"user":{"displayName":"Nurul Izzati Darul Zaman","userId":"02130484108045505747"}},"outputId":"67a39def-4237-417c-88fe-04ff4e38799f"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[Row(_c0='03/05/2019 08:06:14', _c1=100, _c2=0.57, _c3=0.51, _c4=47),\n"," Row(_c0='03/05/2019 08:11:14', _c1=100, _c2=0.47, _c3=0.62, _c4=43),\n"," Row(_c0='03/05/2019 08:16:14', _c1=100, _c2=0.56, _c3=0.57, _c4=62),\n"," Row(_c0='03/05/2019 08:21:14', _c1=100, _c2=0.57, _c3=0.56, _c4=50),\n"," Row(_c0='03/05/2019 08:26:14', _c1=100, _c2=0.35, _c3=0.46, _c4=43)]"]},"metadata":{},"execution_count":16}],"source":["df2.head(5)"]},{"cell_type":"markdown","metadata":{"id":"M1c3v0LGY1WK"},"source":["As you can see, we have five rows, but we do not have column names. Because we did not specify a header. So Spark just created column names. Basically used a pattern `_c#`."]},{"cell_type":"markdown","metadata":{"id":"YuG6tj88Y1WL"},"source":["Spark allows us to rename the columns. By using `withColumnRenamed()` method."]},{"cell_type":"code","execution_count":17,"metadata":{"id":"HOtq3EC_Y1WL","executionInfo":{"status":"ok","timestamp":1732774404530,"user_tz":-480,"elapsed":319,"user":{"displayName":"Nurul Izzati Darul Zaman","userId":"02130484108045505747"}}},"outputs":[],"source":["df2 = df2.withColumnRenamed('_c0', 'event_datetime')\\\n","         .withColumnRenamed('_c1','server_id')\\\n","         .withColumnRenamed('_c2','cpu_utilization')\\\n","         .withColumnRenamed('_c3','free_memory')\\\n","         .withColumnRenamed('_c4','session_count')"]},{"cell_type":"markdown","metadata":{"id":"ZqebRzb4Y1WM"},"source":["Here is the new DataFrame in the tabular format."]},{"cell_type":"code","execution_count":18,"metadata":{"id":"f2KPw4XoY1WM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1732774408417,"user_tz":-480,"elapsed":643,"user":{"displayName":"Nurul Izzati Darul Zaman","userId":"02130484108045505747"}},"outputId":"6688800d-491b-4cff-82df-a997d564cefa"},"outputs":[{"output_type":"stream","name":"stdout","text":["+-------------------+---------+---------------+-----------+-------------+\n","|     event_datetime|server_id|cpu_utilization|free_memory|session_count|\n","+-------------------+---------+---------------+-----------+-------------+\n","|03/05/2019 08:06:14|      100|           0.57|       0.51|           47|\n","|03/05/2019 08:11:14|      100|           0.47|       0.62|           43|\n","|03/05/2019 08:16:14|      100|           0.56|       0.57|           62|\n","|03/05/2019 08:21:14|      100|           0.57|       0.56|           50|\n","|03/05/2019 08:26:14|      100|           0.35|       0.46|           43|\n","|03/05/2019 08:31:14|      100|           0.41|       0.58|           48|\n","|03/05/2019 08:36:14|      100|           0.57|       0.35|           58|\n","|03/05/2019 08:41:14|      100|           0.41|        0.4|           58|\n","|03/05/2019 08:46:14|      100|           0.53|       0.35|           62|\n","|03/05/2019 08:51:14|      100|           0.51|        0.6|           45|\n","|03/05/2019 08:56:14|      100|           0.32|       0.37|           47|\n","|03/05/2019 09:01:14|      100|           0.62|       0.59|           60|\n","|03/05/2019 09:06:14|      100|           0.66|       0.72|           57|\n","|03/05/2019 09:11:14|      100|           0.54|       0.54|           44|\n","|03/05/2019 09:16:14|      100|           0.29|        0.4|           47|\n","|03/05/2019 09:21:14|      100|           0.43|       0.68|           66|\n","|03/05/2019 09:26:14|      100|           0.49|       0.66|           65|\n","|03/05/2019 09:31:14|      100|           0.64|       0.55|           66|\n","|03/05/2019 09:36:14|      100|           0.42|        0.6|           42|\n","|03/05/2019 09:41:14|      100|           0.55|       0.59|           63|\n","+-------------------+---------+---------------+-----------+-------------+\n","only showing top 20 rows\n","\n"]}],"source":["df2.show()"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"37nJGtkjY1WN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1732774411066,"user_tz":-480,"elapsed":389,"user":{"displayName":"Nurul Izzati Darul Zaman","userId":"02130484108045505747"}},"outputId":"90bd0de1-c996-4581-a170-f5c0042b43dd"},"outputs":[{"output_type":"stream","name":"stdout","text":["root\n"," |-- event_datetime: string (nullable = true)\n"," |-- server_id: integer (nullable = true)\n"," |-- cpu_utilization: double (nullable = true)\n"," |-- free_memory: double (nullable = true)\n"," |-- session_count: integer (nullable = true)\n","\n"]}],"source":["df2.printSchema()"]},{"cell_type":"markdown","metadata":{"id":"St4ncbDQY1WN"},"source":["Another useful method in DataFrame API is `describe()` that computes basic statistics for numeric and string columns.\n","\n","This include count, mean, stddev, min, and max."]},{"cell_type":"code","execution_count":20,"metadata":{"id":"9PRhKDHnY1WN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1732774414181,"user_tz":-480,"elapsed":2586,"user":{"displayName":"Nurul Izzati Darul Zaman","userId":"02130484108045505747"}},"outputId":"7b673578-5faf-42b2-95d2-078f62fad11d"},"outputs":[{"output_type":"stream","name":"stdout","text":["+-------+-------------------+\n","|summary|    cpu_utilization|\n","+-------+-------------------+\n","|  count|             500000|\n","|   mean| 0.6205177399999797|\n","| stddev|0.15875173872913106|\n","|    min|               0.22|\n","|    max|                1.0|\n","+-------+-------------------+\n","\n"]}],"source":["df2.describe('cpu_utilization').show()"]},{"cell_type":"markdown","metadata":{"id":"yEo6bjJGY1WO"},"source":["If no columns are given, this function computes statistics for all numerical or string columns."]},{"cell_type":"code","execution_count":21,"metadata":{"id":"lhSWq_DmY1WO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1732774423866,"user_tz":-480,"elapsed":8450,"user":{"displayName":"Nurul Izzati Darul Zaman","userId":"02130484108045505747"}},"outputId":"811dcb35-5d5a-4fd0-afe0-50a26b80b03f"},"outputs":[{"output_type":"stream","name":"stdout","text":["+-------+-------------------+------------------+-------------------+-------------------+------------------+\n","|summary|     event_datetime|         server_id|    cpu_utilization|        free_memory|     session_count|\n","+-------+-------------------+------------------+-------------------+-------------------+------------------+\n","|  count|             500000|            500000|             500000|             500000|            500000|\n","|   mean|               NULL|             124.5| 0.6205177399999797|0.37912809999999125|          69.59616|\n","| stddev|               NULL|14.430884120552715|0.15875173872913106|0.15830931278376184|14.850676696352798|\n","|    min|03/05/2019 08:06:14|               100|               0.22|                0.0|                32|\n","|    max|04/09/2019 01:22:46|               149|                1.0|               0.78|               105|\n","+-------+-------------------+------------------+-------------------+-------------------+------------------+\n","\n"]}],"source":["df2.describe().show()"]},{"cell_type":"code","source":["df2.describe(['free_memory','session_count']).show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1Tqh1LSK5L6v","executionInfo":{"status":"ok","timestamp":1732774425397,"user_tz":-480,"elapsed":1533,"user":{"displayName":"Nurul Izzati Darul Zaman","userId":"02130484108045505747"}},"outputId":"44c73054-592b-461c-8980-db4597cf5b0d"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["+-------+-------------------+------------------+\n","|summary|        free_memory|     session_count|\n","+-------+-------------------+------------------+\n","|  count|             500000|            500000|\n","|   mean|0.37912809999999125|          69.59616|\n","| stddev|0.15830931278376184|14.850676696352798|\n","|    min|                0.0|                32|\n","|    max|               0.78|               105|\n","+-------+-------------------+------------------+\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"nQTVVi0mY1WX"},"source":["### Load a JSON file into a DataFrame\n","In the following cell, we are trying to load a JSON file into a DataFrame by using `spark.read` command."]},{"cell_type":"code","execution_count":23,"metadata":{"id":"faLdxbLVY1WX","executionInfo":{"status":"ok","timestamp":1732774427046,"user_tz":-480,"elapsed":318,"user":{"displayName":"Nurul Izzati Darul Zaman","userId":"02130484108045505747"}}},"outputs":[],"source":["file_path =MAIN_DIRECTORY+\"/data/utilization.json\""]},{"cell_type":"code","execution_count":24,"metadata":{"id":"1H6IfOBwY1WY","executionInfo":{"status":"ok","timestamp":1732774449921,"user_tz":-480,"elapsed":3796,"user":{"displayName":"Nurul Izzati Darul Zaman","userId":"02130484108045505747"}}},"outputs":[],"source":["df3 = spark.read.format(\"json\").load(file_path)"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"1vI3rr1EY1WY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1732774452024,"user_tz":-480,"elapsed":316,"user":{"displayName":"Nurul Izzati Darul Zaman","userId":"02130484108045505747"}},"outputId":"f19d5540-6f84-4b7e-bb38-9ab5cdc2b26b"},"outputs":[{"output_type":"stream","name":"stdout","text":["+---------------+-------------------+-----------+---------+-------------+\n","|cpu_utilization|     event_datetime|free_memory|server_id|session_count|\n","+---------------+-------------------+-----------+---------+-------------+\n","|           0.57|03/05/2019 08:06:14|       0.51|      100|           47|\n","|           0.47|03/05/2019 08:11:14|       0.62|      100|           43|\n","|           0.56|03/05/2019 08:16:14|       0.57|      100|           62|\n","|           0.57|03/05/2019 08:21:14|       0.56|      100|           50|\n","|           0.35|03/05/2019 08:26:14|       0.46|      100|           43|\n","|           0.41|03/05/2019 08:31:14|       0.58|      100|           48|\n","|           0.57|03/05/2019 08:36:14|       0.35|      100|           58|\n","|           0.41|03/05/2019 08:41:14|        0.4|      100|           58|\n","|           0.53|03/05/2019 08:46:14|       0.35|      100|           62|\n","|           0.51|03/05/2019 08:51:14|        0.6|      100|           45|\n","|           0.32|03/05/2019 08:56:14|       0.37|      100|           47|\n","|           0.62|03/05/2019 09:01:14|       0.59|      100|           60|\n","|           0.66|03/05/2019 09:06:14|       0.72|      100|           57|\n","|           0.54|03/05/2019 09:11:14|       0.54|      100|           44|\n","|           0.29|03/05/2019 09:16:14|        0.4|      100|           47|\n","|           0.43|03/05/2019 09:21:14|       0.68|      100|           66|\n","|           0.49|03/05/2019 09:26:14|       0.66|      100|           65|\n","|           0.64|03/05/2019 09:31:14|       0.55|      100|           66|\n","|           0.42|03/05/2019 09:36:14|        0.6|      100|           42|\n","|           0.55|03/05/2019 09:41:14|       0.59|      100|           63|\n","+---------------+-------------------+-----------+---------+-------------+\n","only showing top 20 rows\n","\n"]}],"source":["df3.show()"]},{"cell_type":"code","execution_count":26,"metadata":{"id":"BODrjy2pY1WY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1732774455720,"user_tz":-480,"elapsed":1278,"user":{"displayName":"Nurul Izzati Darul Zaman","userId":"02130484108045505747"}},"outputId":"dcd7f4a2-ae1e-48a4-c317-44d21bb5bcc2"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["153279"]},"metadata":{},"execution_count":26}],"source":["df3.count()"]},{"cell_type":"markdown","metadata":{"id":"CB-zlSblY1WZ"},"source":["Now, what you will notice here is we did not have to change column names.That is because in JSON, you specify key-value pairs. For example, there was a row that has `cpu_utilization` equals to 0.77, that corresponds to the first row. This row also has a key-value pair with `free_memory` equals to 0.22 and `server_id` equals to 115."]},{"cell_type":"markdown","metadata":{"id":"gWZwwCBVY1WZ"},"source":["Apache Spark provides an attribute called `columns`, to show the list of a DataFrame's columns."]},{"cell_type":"code","execution_count":27,"metadata":{"id":"p3c0dgl_Y1WZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1732774457071,"user_tz":-480,"elapsed":308,"user":{"displayName":"Nurul Izzati Darul Zaman","userId":"02130484108045505747"}},"outputId":"964b7ee0-5eaa-4f46-b2bc-8971b8db4d89"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['cpu_utilization',\n"," 'event_datetime',\n"," 'free_memory',\n"," 'server_id',\n"," 'session_count']"]},"metadata":{},"execution_count":27}],"source":["df3.columns"]},{"cell_type":"markdown","metadata":{"id":"r77OsBRpY1WZ"},"source":["Sometimes we want to work with a subset of data. For example, we have 500000 rows of data in this DataFrame. Although they are not too many rows, it may be more than you want to work with at any particular time. And you would rather work with a sample of the data. To do that, you can use `sample` command."]},{"cell_type":"code","execution_count":28,"metadata":{"id":"xMK4-1JkY1Wa","executionInfo":{"status":"ok","timestamp":1732774458608,"user_tz":-480,"elapsed":349,"user":{"displayName":"Nurul Izzati Darul Zaman","userId":"02130484108045505747"}}},"outputs":[],"source":["df3_sample = df3.sample(False, 0.1)"]},{"cell_type":"code","execution_count":29,"metadata":{"id":"_AqLlE-AY1Wa","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1732774460666,"user_tz":-480,"elapsed":1700,"user":{"displayName":"Nurul Izzati Darul Zaman","userId":"02130484108045505747"}},"outputId":"10e2a1f4-b870-4622-b616-3e0b6ffe580e"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["15390"]},"metadata":{},"execution_count":29}],"source":["df3_sample.count()"]},{"cell_type":"markdown","metadata":{"id":"7E95799qY1Wa"},"source":["DataFrame API provides a method called `sort()` to sort the rows based on one or more columns."]},{"cell_type":"code","execution_count":30,"metadata":{"id":"54Jb65kLY1Wb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1732774466973,"user_tz":-480,"elapsed":2086,"user":{"displayName":"Nurul Izzati Darul Zaman","userId":"02130484108045505747"}},"outputId":"9d9151d6-afd4-43f7-976e-2c22ca659a0f"},"outputs":[{"output_type":"stream","name":"stdout","text":["+---------------+-------------------+-----------+---------+-------------+\n","|cpu_utilization|     event_datetime|free_memory|server_id|session_count|\n","+---------------+-------------------+-----------+---------+-------------+\n","|           0.57|03/05/2019 08:06:14|       0.51|      100|           47|\n","|           0.47|03/05/2019 08:11:14|       0.62|      100|           43|\n","|           0.56|03/05/2019 08:16:14|       0.57|      100|           62|\n","|           0.57|03/05/2019 08:21:14|       0.56|      100|           50|\n","|           0.35|03/05/2019 08:26:14|       0.46|      100|           43|\n","|           0.41|03/05/2019 08:31:14|       0.58|      100|           48|\n","|           0.57|03/05/2019 08:36:14|       0.35|      100|           58|\n","|           0.41|03/05/2019 08:41:14|        0.4|      100|           58|\n","|           0.53|03/05/2019 08:46:14|       0.35|      100|           62|\n","|           0.51|03/05/2019 08:51:14|        0.6|      100|           45|\n","|           0.32|03/05/2019 08:56:14|       0.37|      100|           47|\n","|           0.62|03/05/2019 09:01:14|       0.59|      100|           60|\n","|           0.66|03/05/2019 09:06:14|       0.72|      100|           57|\n","|           0.54|03/05/2019 09:11:14|       0.54|      100|           44|\n","|           0.29|03/05/2019 09:16:14|        0.4|      100|           47|\n","|           0.43|03/05/2019 09:21:14|       0.68|      100|           66|\n","|           0.49|03/05/2019 09:26:14|       0.66|      100|           65|\n","|           0.64|03/05/2019 09:31:14|       0.55|      100|           66|\n","|           0.42|03/05/2019 09:36:14|        0.6|      100|           42|\n","|           0.55|03/05/2019 09:41:14|       0.59|      100|           63|\n","+---------------+-------------------+-----------+---------+-------------+\n","only showing top 20 rows\n","\n"]}],"source":["df3_sort = df3.sort('server_id').show()"]},{"cell_type":"markdown","metadata":{"id":"V4OZbMCjY1Wb"},"source":["If we want to sort the rows based on more that one coulmn, we can specify the list of columns and sorting order by using the following syntax."]},{"cell_type":"code","execution_count":31,"metadata":{"id":"QhEV0Ay2Y1Wb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1732774468387,"user_tz":-480,"elapsed":1072,"user":{"displayName":"Nurul Izzati Darul Zaman","userId":"02130484108045505747"}},"outputId":"693314de-a676-4c34-f229-e866254b724f"},"outputs":[{"output_type":"stream","name":"stdout","text":["+---------------+-------------------+-----------+---------+-------------+\n","|cpu_utilization|     event_datetime|free_memory|server_id|session_count|\n","+---------------+-------------------+-----------+---------+-------------+\n","|            0.6|04/09/2019 01:21:40|       0.62|      114|           44|\n","|           0.74|04/09/2019 01:21:38|       0.34|      113|           74|\n","|           0.56|04/09/2019 01:21:36|       0.09|      112|           83|\n","|           0.75|04/09/2019 01:21:34|       0.29|      111|           74|\n","|           0.36|04/09/2019 01:21:33|       0.27|      110|           55|\n","|           0.64|04/09/2019 01:21:31|       0.32|      109|           76|\n","|           0.57|04/09/2019 01:21:29|       0.08|      108|           66|\n","|           0.85|04/09/2019 01:21:28|       0.46|      107|           81|\n","|           0.28|04/09/2019 01:21:26|       0.64|      106|           56|\n","|           0.49|04/09/2019 01:21:24|       0.69|      105|           73|\n","|           0.66|04/09/2019 01:21:23|       0.17|      104|           63|\n","|           0.62|04/09/2019 01:21:21|       0.11|      103|           87|\n","|           0.83|04/09/2019 01:21:19|        0.1|      102|           92|\n","|           0.89|04/09/2019 01:21:17|        0.0|      101|          101|\n","|           0.38|04/09/2019 01:21:16|       0.41|      100|           50|\n","|           0.63|04/09/2019 01:16:40|       0.59|      114|           71|\n","|           0.69|04/09/2019 01:16:38|        0.1|      113|           77|\n","|           0.83|04/09/2019 01:16:36|       0.36|      112|           84|\n","|            0.7|04/09/2019 01:16:34|       0.24|      111|           57|\n","|           0.74|04/09/2019 01:16:33|       0.54|      110|           54|\n","+---------------+-------------------+-----------+---------+-------------+\n","only showing top 20 rows\n","\n"]}],"source":["df3_sorted = df3.sort(['event_datetime','server_id'], ascending = [0,1]).show()"]},{"cell_type":"markdown","metadata":{"id":"zsR5TZrtY1Wb"},"source":["### Filtering using DataFrame API"]},{"cell_type":"markdown","metadata":{"id":"TJ3iqTuzY1Wc"},"source":["Now, let's take a look at how we can use DataFrame API to filter some of the rows in DataFrames.\n","\n","One of the DataFrames that we have created is `df1`, which stores location ID, and temperature measurement at a particular point and time."]},{"cell_type":"code","execution_count":32,"metadata":{"id":"dwBgIpKoY1Wc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1732774470061,"user_tz":-480,"elapsed":331,"user":{"displayName":"Nurul Izzati Darul Zaman","userId":"02130484108045505747"}},"outputId":"b399c313-ea98-48fc-9937-6e9297709089"},"outputs":[{"output_type":"stream","name":"stdout","text":["+-------------------+-----------+------------+\n","|         event_date|location_id|temp_celcius|\n","+-------------------+-----------+------------+\n","|03/04/2019 19:48:06|       loc0|          29|\n","|03/04/2019 19:53:06|       loc0|          27|\n","|03/04/2019 19:58:06|       loc0|          28|\n","|03/04/2019 20:03:06|       loc0|          30|\n","|03/04/2019 20:08:06|       loc0|          27|\n","+-------------------+-----------+------------+\n","only showing top 5 rows\n","\n"]}],"source":["df1.show(5)"]},{"cell_type":"markdown","metadata":{"id":"9fNyvG00Y1Wc"},"source":["If we want to filter rows based on their `location_id`, we can use `filter` command. `filter(condition)` filters rows using the given condition. `filter()` method essentially allows us to specify a `WHERE` clause."]},{"cell_type":"code","execution_count":33,"metadata":{"id":"oGj4qVlNY1Wc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1732774471909,"user_tz":-480,"elapsed":330,"user":{"displayName":"Nurul Izzati Darul Zaman","userId":"02130484108045505747"}},"outputId":"d1a9516e-e3ea-48ec-e894-c0e3cfea2833"},"outputs":[{"output_type":"stream","name":"stdout","text":["+-------------------+-----------+------------+\n","|         event_date|location_id|temp_celcius|\n","+-------------------+-----------+------------+\n","|03/04/2019 19:48:06|       loc0|          29|\n","|03/04/2019 19:53:06|       loc0|          27|\n","|03/04/2019 19:58:06|       loc0|          28|\n","|03/04/2019 20:03:06|       loc0|          30|\n","|03/04/2019 20:08:06|       loc0|          27|\n","|03/04/2019 20:13:06|       loc0|          27|\n","|03/04/2019 20:18:06|       loc0|          27|\n","|03/04/2019 20:23:06|       loc0|          29|\n","|03/04/2019 20:28:06|       loc0|          32|\n","|03/04/2019 20:33:06|       loc0|          35|\n","|03/04/2019 20:38:06|       loc0|          32|\n","|03/04/2019 20:43:06|       loc0|          28|\n","|03/04/2019 20:48:06|       loc0|          28|\n","|03/04/2019 20:53:06|       loc0|          32|\n","|03/04/2019 20:58:06|       loc0|          34|\n","|03/04/2019 21:03:06|       loc0|          33|\n","|03/04/2019 21:08:06|       loc0|          27|\n","|03/04/2019 21:13:06|       loc0|          28|\n","|03/04/2019 21:18:06|       loc0|          33|\n","|03/04/2019 21:23:06|       loc0|          28|\n","+-------------------+-----------+------------+\n","only showing top 20 rows\n","\n"]}],"source":["df1.filter(df1['location_id']=='loc0').show()"]},{"cell_type":"markdown","metadata":{"id":"BuBsXiMjY1Wd"},"source":["If we want to count all the rows that are located in a specific `location_id`,we can specify the `count()` command."]},{"cell_type":"code","execution_count":34,"metadata":{"id":"9zepo_w3Y1Wd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1732774475859,"user_tz":-480,"elapsed":1360,"user":{"displayName":"Nurul Izzati Darul Zaman","userId":"02130484108045505747"}},"outputId":"f649c83f-9bd7-4256-c0e0-af8094956d2f"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["1000"]},"metadata":{},"execution_count":34}],"source":["df1.filter(df1['location_id']=='loc0').count()"]},{"cell_type":"markdown","metadata":{"id":"HM8VK2p2Y1Wd"},"source":["Sometimes we only need to list one or two columns; in this case, we can use `select()` method that projects a set of expressions and returns a new DataFrame. Let's take a look at how it works."]},{"cell_type":"code","execution_count":35,"metadata":{"id":"sKDWhBr-Y1We","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1732774475860,"user_tz":-480,"elapsed":8,"user":{"displayName":"Nurul Izzati Darul Zaman","userId":"02130484108045505747"}},"outputId":"0d92b8f9-4a41-45eb-d698-ab650ef3bed8"},"outputs":[{"output_type":"stream","name":"stdout","text":["+-----------+------------+\n","|location_id|temp_celcius|\n","+-----------+------------+\n","|       loc0|          29|\n","|       loc0|          27|\n","|       loc0|          28|\n","|       loc0|          30|\n","|       loc0|          27|\n","|       loc0|          27|\n","|       loc0|          27|\n","|       loc0|          29|\n","|       loc0|          32|\n","|       loc0|          35|\n","|       loc0|          32|\n","|       loc0|          28|\n","|       loc0|          28|\n","|       loc0|          32|\n","|       loc0|          34|\n","|       loc0|          33|\n","|       loc0|          27|\n","|       loc0|          28|\n","|       loc0|          33|\n","|       loc0|          28|\n","+-----------+------------+\n","only showing top 20 rows\n","\n"]}],"source":["df1.select('location_id','temp_celcius').show()"]},{"cell_type":"markdown","metadata":{"id":"TGvXVaLeY1We"},"source":["### Aggregation using DataFrame API"]},{"cell_type":"markdown","metadata":{"id":"W7nWQFuDY1We"},"source":["Now, let's take a look at aggregating using the DataFrame API. In the following cell we will use `groupBy` method that groups the DataFrame using the specified columns, so we can run aggregation on them."]},{"cell_type":"code","execution_count":36,"metadata":{"id":"gsWsvinbY1We","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1732774480176,"user_tz":-480,"elapsed":2127,"user":{"displayName":"Nurul Izzati Darul Zaman","userId":"02130484108045505747"}},"outputId":"5a369948-1102-46a5-85b2-f65c7496b029"},"outputs":[{"output_type":"stream","name":"stdout","text":["+-----------+-----+\n","|location_id|count|\n","+-----------+-----+\n","|     loc196| 1000|\n","|     loc226| 1000|\n","|     loc150| 1000|\n","|     loc292| 1000|\n","|     loc311| 1000|\n","|      loc22| 1000|\n","|      loc31| 1000|\n","|     loc305| 1000|\n","|      loc82| 1000|\n","|      loc90| 1000|\n","|     loc118| 1000|\n","|     loc195| 1000|\n","|     loc208| 1000|\n","|      loc39| 1000|\n","|      loc75| 1000|\n","|     loc228| 1000|\n","|     loc203| 1000|\n","|     loc193| 1000|\n","|     loc122| 1000|\n","|     loc145| 1000|\n","+-----------+-----+\n","only showing top 20 rows\n","\n"]}],"source":["df1.groupBy('location_id').count().show()"]},{"cell_type":"markdown","metadata":{"id":"X7gk4TMPY1Wf"},"source":["If we want to sort the DataFrame, we can use `orderBy` that returns a new DataFrame sorted by the specified column(s)."]},{"cell_type":"code","execution_count":37,"metadata":{"id":"HyCwYsngY1Wf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1732774492503,"user_tz":-480,"elapsed":1554,"user":{"displayName":"Nurul Izzati Darul Zaman","userId":"02130484108045505747"}},"outputId":"f6f0e3b0-ada0-4ece-b81c-fa0c0d6b0b3e"},"outputs":[{"output_type":"stream","name":"stdout","text":["+-------------------+-----------+------------+\n","|         event_date|location_id|temp_celcius|\n","+-------------------+-----------+------------+\n","|03/04/2019 19:48:06|       loc0|          29|\n","|03/04/2019 19:53:06|       loc0|          27|\n","|03/04/2019 19:58:06|       loc0|          28|\n","|03/04/2019 20:03:06|       loc0|          30|\n","|03/04/2019 20:08:06|       loc0|          27|\n","|03/04/2019 20:13:06|       loc0|          27|\n","|03/04/2019 20:18:06|       loc0|          27|\n","|03/04/2019 20:23:06|       loc0|          29|\n","|03/04/2019 20:28:06|       loc0|          32|\n","|03/04/2019 20:33:06|       loc0|          35|\n","|03/04/2019 20:38:06|       loc0|          32|\n","|03/04/2019 20:43:06|       loc0|          28|\n","|03/04/2019 20:48:06|       loc0|          28|\n","|03/04/2019 20:53:06|       loc0|          32|\n","|03/04/2019 20:58:06|       loc0|          34|\n","|03/04/2019 21:03:06|       loc0|          33|\n","|03/04/2019 21:08:06|       loc0|          27|\n","|03/04/2019 21:13:06|       loc0|          28|\n","|03/04/2019 21:18:06|       loc0|          33|\n","|03/04/2019 21:23:06|       loc0|          28|\n","+-------------------+-----------+------------+\n","only showing top 20 rows\n","\n"]}],"source":["df1.orderBy('location_id').show()"]},{"cell_type":"markdown","metadata":{"id":"G_PQpe4iY1Wf"},"source":["To calculate the average temperature at each location, we can use `agg` operation. Let's take a look at the following example."]},{"cell_type":"code","execution_count":38,"metadata":{"id":"Cxpwgu3hY1Wf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1732774495922,"user_tz":-480,"elapsed":2317,"user":{"displayName":"Nurul Izzati Darul Zaman","userId":"02130484108045505747"}},"outputId":"2fc11327-492e-4fdd-c8fe-70d4a9627b1e"},"outputs":[{"output_type":"stream","name":"stdout","text":["+-----------+-----------------+\n","|location_id|avg(temp_celcius)|\n","+-----------+-----------------+\n","|       loc0|           29.176|\n","|       loc1|           28.246|\n","|      loc10|           25.337|\n","|     loc100|           27.297|\n","|     loc101|           25.317|\n","|     loc102|           30.327|\n","|     loc103|           25.341|\n","|     loc104|           26.204|\n","|     loc105|           26.217|\n","|     loc106|           27.201|\n","|     loc107|           33.268|\n","|     loc108|           32.195|\n","|     loc109|           24.138|\n","|      loc11|           25.308|\n","|     loc110|           26.239|\n","|     loc111|           31.391|\n","|     loc112|           33.359|\n","|     loc113|           30.345|\n","|     loc114|           29.261|\n","|     loc115|           23.239|\n","+-----------+-----------------+\n","only showing top 20 rows\n","\n"]}],"source":["df1.groupBy('location_id').agg({'temp_celcius':'mean'}).orderBy('location_id').show()"]},{"cell_type":"markdown","metadata":{"id":"Pz9b9E_-Y1Wf"},"source":["There are different aggregation function options, for example, if we want to have the maximum temperature in each location, we can write the following code."]},{"cell_type":"code","execution_count":39,"metadata":{"id":"ugpx9n2XY1Wf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1732774500340,"user_tz":-480,"elapsed":2457,"user":{"displayName":"Nurul Izzati Darul Zaman","userId":"02130484108045505747"}},"outputId":"778ca32e-fd93-47c4-9405-10ad3d8813d0"},"outputs":[{"output_type":"stream","name":"stdout","text":["+-----------+-----------------+\n","|location_id|max(temp_celcius)|\n","+-----------+-----------------+\n","|      loc99|               40|\n","|      loc98|               39|\n","|      loc97|               38|\n","|      loc96|               35|\n","|      loc95|               40|\n","|      loc94|               32|\n","|      loc93|               31|\n","|      loc92|               40|\n","|      loc91|               37|\n","|      loc90|               30|\n","|       loc9|               39|\n","|      loc89|               37|\n","|      loc88|               32|\n","|      loc87|               38|\n","|      loc86|               30|\n","|      loc85|               35|\n","|      loc84|               33|\n","|      loc83|               33|\n","|      loc82|               34|\n","|      loc81|               30|\n","+-----------+-----------------+\n","only showing top 20 rows\n","\n"]}],"source":["df1.groupBy('location_id').agg({'temp_celcius':'max'}).orderBy('location_id', ascending=False).show()"]},{"cell_type":"markdown","metadata":{"id":"lk4gV8TlY1Wg"},"source":["### Data Sampling"]},{"cell_type":"markdown","metadata":{"id":"4I2kny8lY1Wg"},"source":["Sometimes, we may want to use sampling, particularly when we have large data sets, and we are doing kind of an exploratory analysis. We want to get kind of an understanding at a high level of what the data is like. Sampling can be beneficial for doing quick operations. Now, let's see how we can take a sample, or a subset of that, but randomly. In PySpark, `sample()` method returns a sampled subset of this DataFrame, and it usually takes two parameters, `fraction` that specifies the fraction of rows to generate, range [0.0, 1.0]. The second parameter is `withReplacement`, which is a boolean parameter. Usually, we assign `false` to it, in this case, what that means is each time we pull a row out of our sampling, we don't put it back in, so we will never get duplicates, we will always get unique values."]},{"cell_type":"code","execution_count":40,"metadata":{"id":"RFJfAGMbY1Wg","executionInfo":{"status":"ok","timestamp":1732774620352,"user_tz":-480,"elapsed":1367,"user":{"displayName":"Nurul Izzati Darul Zaman","userId":"02130484108045505747"}}},"outputs":[],"source":["df1_sample1 = df1.sample(fraction=0.1, withReplacement=False)"]},{"cell_type":"code","execution_count":41,"metadata":{"id":"bllHL-5bY1Wg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1732774621176,"user_tz":-480,"elapsed":826,"user":{"displayName":"Nurul Izzati Darul Zaman","userId":"02130484108045505747"}},"outputId":"e54fd41f-64c7-4f2b-e5d1-e533752f277f"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["50454"]},"metadata":{},"execution_count":41}],"source":["df1_sample1.count()"]},{"cell_type":"markdown","metadata":{"id":"kZ55-_wIY1Wh"},"source":["Now, let's run some simple descriptive statistics on our sample."]},{"cell_type":"code","execution_count":42,"metadata":{"id":"S-OD0S2nY1Wh","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1732774623818,"user_tz":-480,"elapsed":2643,"user":{"displayName":"Nurul Izzati Darul Zaman","userId":"02130484108045505747"}},"outputId":"345f0711-43e8-486d-a58b-3ca6652cc6f4"},"outputs":[{"output_type":"stream","name":"stdout","text":["+-----------+------------------+\n","|location_id| avg(temp_celcius)|\n","+-----------+------------------+\n","|       loc0|29.063157894736843|\n","|       loc1|28.666666666666668|\n","|      loc10| 25.45945945945946|\n","|     loc100|27.060606060606062|\n","|     loc101|25.147368421052633|\n","|     loc102|              30.4|\n","|     loc103|25.409523809523808|\n","|     loc104|26.021052631578947|\n","|     loc105| 26.05217391304348|\n","|     loc106|27.413793103448278|\n","+-----------+------------------+\n","only showing top 10 rows\n","\n"]}],"source":["df1_sample1.groupBy('location_id').agg({'temp_celcius':'mean'}).orderBy('location_id').show(10)"]},{"cell_type":"markdown","metadata":{"id":"-t0QHJr2Y1Wh"},"source":["Now, let's compare these results to results of the original data set, the DataFrame `df1`, which has 500000 rows."]},{"cell_type":"code","execution_count":43,"metadata":{"id":"CNGy_qfSY1Wi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1732774625321,"user_tz":-480,"elapsed":1512,"user":{"displayName":"Nurul Izzati Darul Zaman","userId":"02130484108045505747"}},"outputId":"ab3cb48c-a269-4750-ed7a-121eb4c41746"},"outputs":[{"output_type":"stream","name":"stdout","text":["+-----------+-----------------+\n","|location_id|avg(temp_celcius)|\n","+-----------+-----------------+\n","|       loc0|           29.176|\n","|       loc1|           28.246|\n","|      loc10|           25.337|\n","|     loc100|           27.297|\n","|     loc101|           25.317|\n","|     loc102|           30.327|\n","|     loc103|           25.341|\n","|     loc104|           26.204|\n","|     loc105|           26.217|\n","|     loc106|           27.201|\n","+-----------+-----------------+\n","only showing top 10 rows\n","\n"]}],"source":["df1.groupBy('location_id').agg({'temp_celcius':'mean'}).orderBy('location_id').show(10)"]},{"cell_type":"markdown","metadata":{"id":"yBjfYuWqY1Wi"},"source":["As you can see, when we did the sampling and took 10% when we took the average of location zero, we got something that was about 29.4, but the actual is approximately 29.18. Therefore, we can see by sampling, we get very close to what the average is for the actual population. One of the things to consider is the size of the sample that we are drawing."]},{"cell_type":"markdown","metadata":{"id":"OvmOo8mqY1Wi"},"source":["### Save Data from DataFrame"]},{"cell_type":"markdown","metadata":{"id":"yUcDeJygY1Wj"},"source":["Sometimes after we have been working with DataFrames and creating new DataFrames and running calculations and doing sampling, we might want to save our results out. To do this, we can use `write` object and specify the `csv()` method within that, and then specify a name or what we'd like to save. It saves the DataFrame to disk using the csv format."]},{"cell_type":"code","execution_count":44,"metadata":{"id":"LYQ8xACRY1Wj","executionInfo":{"status":"ok","timestamp":1732774628997,"user_tz":-480,"elapsed":2459,"user":{"displayName":"Nurul Izzati Darul Zaman","userId":"02130484108045505747"}}},"outputs":[],"source":["df1.write.csv('df1.csv')"]},{"cell_type":"markdown","metadata":{"id":"Vcy-jSqmY1Wj"},"source":["Now, let's take a look at the current directory"]},{"cell_type":"code","execution_count":45,"metadata":{"id":"At1IL3-OY1Wj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1732774628997,"user_tz":-480,"elapsed":8,"user":{"displayName":"Nurul Izzati Darul Zaman","userId":"02130484108045505747"}},"outputId":"0ca4d965-9419-4a3f-de22-3ed42e62672e"},"outputs":[{"output_type":"stream","name":"stdout","text":["part-00000-4d4e9208-5830-4e4f-8d8d-c3dece1935f9-c000.csv  _SUCCESS\n","part-00001-4d4e9208-5830-4e4f-8d8d-c3dece1935f9-c000.csv\n"]}],"source":["! ls df1.csv # in windows, you would use dir command"]},{"cell_type":"markdown","metadata":{"id":"zJ5a9V1PY1Wk"},"source":["Now, what you will notice here is that `df1.csv` is not a single file. It is a directory. And what is in that directory is four different files with `csv` extensions, and that is because of the way Apache Spark works internally. Spark can break up DataFrames into partition subsets, and in this case, there were four partitions. Each partition has its own file. There is also a `_SUCCESS` flag that was written out. Now, let's list the contents of one of these files."]},{"cell_type":"code","execution_count":46,"metadata":{"id":"J7NvOCNLY1Wk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1732774628997,"user_tz":-480,"elapsed":3,"user":{"displayName":"Nurul Izzati Darul Zaman","userId":"02130484108045505747"}},"outputId":"46c40aea-b797-42d5-cc8d-51febee73fc1"},"outputs":[{"output_type":"stream","name":"stdout","text":["head: cannot open 'df1.csv/part-00000-5cb7c640-19b1-48b7-b3e5-ce37cca11d95-c000.csv' for reading: No such file or directory\n"]}],"source":["! head df1.csv/part-00000-5cb7c640-19b1-48b7-b3e5-ce37cca11d95-c000.csv\n","\n"," # for Windows, use more command"]},{"cell_type":"markdown","metadata":{"id":"_WFVYw-CY1Wk"},"source":["To write the DataFrame in JSON format, you can use the following code."]},{"cell_type":"code","execution_count":47,"metadata":{"id":"Zd4TQyIuY1Wl","executionInfo":{"status":"ok","timestamp":1732774633583,"user_tz":-480,"elapsed":4020,"user":{"displayName":"Nurul Izzati Darul Zaman","userId":"02130484108045505747"}}},"outputs":[],"source":["df1.write.json('df1.json')"]},{"cell_type":"code","execution_count":48,"metadata":{"id":"7aq5odrMY1Wl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1732774633584,"user_tz":-480,"elapsed":21,"user":{"displayName":"Nurul Izzati Darul Zaman","userId":"02130484108045505747"}},"outputId":"1e85667c-61fa-4939-8e9e-93216e3ea60c"},"outputs":[{"output_type":"stream","name":"stdout","text":["part-00000-6cf7019f-4ee8-44d5-8ced-33e7def79031-c000.json  _SUCCESS\n","part-00001-6cf7019f-4ee8-44d5-8ced-33e7def79031-c000.json\n"]}],"source":["!ls df1.json"]},{"cell_type":"code","execution_count":49,"metadata":{"id":"1N9GHSunY1Wl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1732774633584,"user_tz":-480,"elapsed":14,"user":{"displayName":"Nurul Izzati Darul Zaman","userId":"02130484108045505747"}},"outputId":"40b7dee3-42f2-4010-de8b-ccd97ae7ce83"},"outputs":[{"output_type":"stream","name":"stdout","text":["head: cannot open 'df1.json/part-00000-e3666b64-84b9-4168-8c6c-0ea6c727660c-c000.json' for reading: No such file or directory\n"]}],"source":["! head df1.json/part-00000-e3666b64-84b9-4168-8c6c-0ea6c727660c-c000.json"]},{"cell_type":"markdown","metadata":{"id":"BfxVENqAY1Wm"},"source":["## Querying DataFrames with SQL"]},{"cell_type":"markdown","metadata":{"id":"OOj5kXlcY1Wm"},"source":["Up to now, we've been using the Spark DataFrame API to work with DataFrames. Now, we're going to switch gears and we're going to work with SQL. In particular, we're going to use Spark SQL for working with DataFrames."]},{"cell_type":"markdown","metadata":{"id":"NUq-bZVtY1Wm"},"source":["In this part, we will use `utilization.json` that includes cpu utilization, the amount of free memory at a particular point in time, and then the number of sessions that are currently connected to the server at the particular point in time."]},{"cell_type":"code","execution_count":50,"metadata":{"id":"Nh8WERryY1Wm","executionInfo":{"status":"ok","timestamp":1732774634936,"user_tz":-480,"elapsed":1357,"user":{"displayName":"Nurul Izzati Darul Zaman","userId":"02130484108045505747"}}},"outputs":[],"source":["file_path =MAIN_DIRECTORY+\"/data/utilization.json\"\n","df_util = spark.read.format('json').load(file_path)"]},{"cell_type":"code","execution_count":53,"metadata":{"id":"BwceR_UYY1Wn","executionInfo":{"status":"ok","timestamp":1732774679331,"user_tz":-480,"elapsed":3790,"user":{"displayName":"Nurul Izzati Darul Zaman","userId":"02130484108045505747"}}},"outputs":[],"source":["file_path =MAIN_DIRECTORY+\"/data/utilization.csv\"\n","df_util = spark.read.format('csv').option('inferSchema', 'true').load(file_path)"]},{"cell_type":"code","execution_count":54,"metadata":{"scrolled":true,"id":"lFF0NQWgY1Wn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1732774683204,"user_tz":-480,"elapsed":380,"user":{"displayName":"Nurul Izzati Darul Zaman","userId":"02130484108045505747"}},"outputId":"afdb3c70-ba44-4b65-ce2e-9ecbf8d73020"},"outputs":[{"output_type":"stream","name":"stdout","text":["+-------------------+---+----+----+---+\n","|                _c0|_c1| _c2| _c3|_c4|\n","+-------------------+---+----+----+---+\n","|03/05/2019 08:06:14|100|0.57|0.51| 47|\n","|03/05/2019 08:11:14|100|0.47|0.62| 43|\n","|03/05/2019 08:16:14|100|0.56|0.57| 62|\n","|03/05/2019 08:21:14|100|0.57|0.56| 50|\n","|03/05/2019 08:26:14|100|0.35|0.46| 43|\n","|03/05/2019 08:31:14|100|0.41|0.58| 48|\n","|03/05/2019 08:36:14|100|0.57|0.35| 58|\n","|03/05/2019 08:41:14|100|0.41| 0.4| 58|\n","|03/05/2019 08:46:14|100|0.53|0.35| 62|\n","|03/05/2019 08:51:14|100|0.51| 0.6| 45|\n","+-------------------+---+----+----+---+\n","only showing top 10 rows\n","\n"]}],"source":["df_util.show(10)"]},{"cell_type":"code","execution_count":55,"metadata":{"id":"HSaThsL8Y1Wn","executionInfo":{"status":"ok","timestamp":1732774687240,"user_tz":-480,"elapsed":311,"user":{"displayName":"Nurul Izzati Darul Zaman","userId":"02130484108045505747"}}},"outputs":[],"source":["df_util = df_util.withColumnRenamed('_c0', 'event_datetime')\\\n","                .withColumnRenamed('_c1', 'server_id')\\\n","                .withColumnRenamed('_c2', 'cpu_utilization')\\\n","                .withColumnRenamed('_c3', 'free_memory')\\\n","                .withColumnRenamed('_c4', 'session_count')"]},{"cell_type":"code","execution_count":56,"metadata":{"id":"e5DnRKjdY1Wo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1732774689833,"user_tz":-480,"elapsed":673,"user":{"displayName":"Nurul Izzati Darul Zaman","userId":"02130484108045505747"}},"outputId":"5b36596f-a5c6-45d5-d819-41d924f92453"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["500000"]},"metadata":{},"execution_count":56}],"source":["df_util.count()"]},{"cell_type":"markdown","metadata":{"id":"twdd0IqlY1Wo"},"source":["To work with SQL in Spark, we have to create a temporary view. And to do that, we specify the DataFrame, and then we call the method `createOrReplaceTempView()` and then we should specify a name for this table. Let's do it."]},{"cell_type":"code","execution_count":57,"metadata":{"id":"KFZfplA8Y1Wo","executionInfo":{"status":"ok","timestamp":1732774691739,"user_tz":-480,"elapsed":315,"user":{"displayName":"Nurul Izzati Darul Zaman","userId":"02130484108045505747"}}},"outputs":[],"source":["df_util.createOrReplaceTempView(\"utilization\")"]},{"cell_type":"markdown","metadata":{"id":"aT1iKEAHY1Wp"},"source":["Now, we have the ability to query on a table called utilization. We will create that by executing a SQL command in the Spark session."]},{"cell_type":"code","execution_count":58,"metadata":{"id":"pAmCBAJuY1Wp","executionInfo":{"status":"ok","timestamp":1732774694101,"user_tz":-480,"elapsed":337,"user":{"displayName":"Nurul Izzati Darul Zaman","userId":"02130484108045505747"}}},"outputs":[],"source":["df_sql = spark.sql(\"SELECT * FROM utilization LIMIT 10\")"]},{"cell_type":"code","execution_count":59,"metadata":{"id":"S3kRONxZY1Wp","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1732774695984,"user_tz":-480,"elapsed":325,"user":{"displayName":"Nurul Izzati Darul Zaman","userId":"02130484108045505747"}},"outputId":"1dc93ee7-1ad4-4a59-8d40-d1a93f7341fb"},"outputs":[{"output_type":"stream","name":"stdout","text":["+-------------------+---------+---------------+-----------+-------------+\n","|     event_datetime|server_id|cpu_utilization|free_memory|session_count|\n","+-------------------+---------+---------------+-----------+-------------+\n","|03/05/2019 08:06:14|      100|           0.57|       0.51|           47|\n","|03/05/2019 08:11:14|      100|           0.47|       0.62|           43|\n","|03/05/2019 08:16:14|      100|           0.56|       0.57|           62|\n","|03/05/2019 08:21:14|      100|           0.57|       0.56|           50|\n","|03/05/2019 08:26:14|      100|           0.35|       0.46|           43|\n","|03/05/2019 08:31:14|      100|           0.41|       0.58|           48|\n","|03/05/2019 08:36:14|      100|           0.57|       0.35|           58|\n","|03/05/2019 08:41:14|      100|           0.41|        0.4|           58|\n","|03/05/2019 08:46:14|      100|           0.53|       0.35|           62|\n","|03/05/2019 08:51:14|      100|           0.51|        0.6|           45|\n","+-------------------+---------+---------------+-----------+-------------+\n","\n"]}],"source":["df_sql.show(20)"]},{"cell_type":"markdown","metadata":{"id":"wA_FLV0MY1Wq"},"source":["If we want to project on specific columns, we can do it in the following way."]},{"cell_type":"code","execution_count":60,"metadata":{"id":"f5LamIThY1Wq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1732774698746,"user_tz":-480,"elapsed":349,"user":{"displayName":"Nurul Izzati Darul Zaman","userId":"02130484108045505747"}},"outputId":"5fd10f33-7007-4fbb-da06-42c092f87ffd"},"outputs":[{"output_type":"stream","name":"stdout","text":["+---------+-----------+\n","|server_id|free_memory|\n","+---------+-----------+\n","|      100|       0.51|\n","|      100|       0.62|\n","|      100|       0.57|\n","|      100|       0.56|\n","|      100|       0.46|\n","|      100|       0.58|\n","|      100|       0.35|\n","|      100|        0.4|\n","|      100|       0.35|\n","|      100|        0.6|\n","+---------+-----------+\n","\n"]}],"source":["spark.sql(\"SELECT server_id, free_memory FROM utilization LIMIT 10\").show()"]},{"cell_type":"markdown","metadata":{"id":"aoU8x9rYY1Wr"},"source":["### Filtering DataFrames with SQL\n","Next, we are going to take a look at how to filter DataFrames using Spark SQL.  "]},{"cell_type":"code","execution_count":61,"metadata":{"id":"CU0m8EEuY1Wr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1732774702805,"user_tz":-480,"elapsed":1383,"user":{"displayName":"Nurul Izzati Darul Zaman","userId":"02130484108045505747"}},"outputId":"75ab0478-75c4-4785-ec40-754c0e674bcd"},"outputs":[{"output_type":"stream","name":"stdout","text":["+-------------------+---------+---------------+-----------+-------------+\n","|     event_datetime|server_id|cpu_utilization|free_memory|session_count|\n","+-------------------+---------+---------------+-----------+-------------+\n","|03/05/2019 08:07:44|      149|           0.74|       0.27|           66|\n","|03/05/2019 08:12:44|      149|            0.9|       0.34|           85|\n","|03/05/2019 08:17:44|      149|           0.59|       0.19|           84|\n","|03/05/2019 08:22:44|      149|            0.6|       0.08|           81|\n","|03/05/2019 08:27:44|      149|           0.83|       0.42|           73|\n","|03/05/2019 08:32:44|      149|           0.75|       0.19|           84|\n","|03/05/2019 08:37:44|      149|            0.9|       0.14|           92|\n","|03/05/2019 08:42:44|      149|           0.67|       0.16|           88|\n","|03/05/2019 08:47:44|      149|           0.91|       0.31|           71|\n","|03/05/2019 08:52:44|      149|           0.82|       0.13|           72|\n","+-------------------+---------+---------------+-----------+-------------+\n","\n"]}],"source":["# Example 1\n","spark.sql(\"SELECT * FROM utilization WHERE server_id = 149 LIMIT 10\").show()"]},{"cell_type":"code","execution_count":62,"metadata":{"id":"JsdqMkIwY1Wr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1732774703166,"user_tz":-480,"elapsed":3,"user":{"displayName":"Nurul Izzati Darul Zaman","userId":"02130484108045505747"}},"outputId":"431b6bb6-6572-440f-bfb1-9802470eca7e"},"outputs":[{"output_type":"stream","name":"stdout","text":["+---+---+\n","|sid| sc|\n","+---+---+\n","|100| 62|\n","|100| 58|\n","|100| 58|\n","|100| 62|\n","|100| 60|\n","|100| 57|\n","|100| 66|\n","|100| 65|\n","|100| 66|\n","|100| 63|\n","+---+---+\n","\n"]}],"source":["# Example 2\n","spark.sql(\"SELECT server_id as sid, session_count as sc \\\n","            FROM utilization WHERE session_count >50 LIMIT 10\").show()"]},{"cell_type":"code","execution_count":63,"metadata":{"id":"AMkWks4kY1Ws","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1732774707620,"user_tz":-480,"elapsed":2035,"user":{"displayName":"Nurul Izzati Darul Zaman","userId":"02130484108045505747"}},"outputId":"c0118386-dac3-4779-ce17-000be11dfb48"},"outputs":[{"output_type":"stream","name":"stdout","text":["+---------+-------------+\n","|server_id|session_count|\n","+---------+-------------+\n","|      120|           80|\n","|      120|           80|\n","|      120|           80|\n","|      120|           80|\n","|      120|           80|\n","|      120|           80|\n","|      120|           80|\n","|      120|           80|\n","|      120|           80|\n","|      120|           80|\n","+---------+-------------+\n","\n"]}],"source":["# Example 3\n","spark.sql(\"SELECT server_id, session_count FROM utilization \\\n","           WHERE session_count >50 AND server_id = 120 \\\n","           ORDER BY session_count DESC \\\n","           LIMIT 10\").show()"]},{"cell_type":"markdown","metadata":{"id":"cZefSzgvY1Ws"},"source":["### Aggregation DataFrames with SQL"]},{"cell_type":"markdown","metadata":{"id":"vE-v5NDlY1Wt"},"source":["When we work with SQL in databases, we often use SQL to perform aggregations and the same holds true when working with SQL in Spark. Let's write some basic queries against the DataFrame and do a very simple aggregations."]},{"cell_type":"code","execution_count":64,"metadata":{"id":"SfCjL5f3Y1Wt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1732774708330,"user_tz":-480,"elapsed":357,"user":{"displayName":"Nurul Izzati Darul Zaman","userId":"02130484108045505747"}},"outputId":"bc3fe5c8-fe4b-44da-ac0d-57d884ca1dfc"},"outputs":[{"output_type":"stream","name":"stdout","text":["+--------+\n","|count(1)|\n","+--------+\n","|  500000|\n","+--------+\n","\n"]}],"source":["# Example 1\n","spark.sql(\"SELECT count(*) FROM utilization\").show()"]},{"cell_type":"code","execution_count":65,"metadata":{"id":"SdmLF9k7Y1Wt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1732774712307,"user_tz":-480,"elapsed":1579,"user":{"displayName":"Nurul Izzati Darul Zaman","userId":"02130484108045505747"}},"outputId":"0a239470-2dbf-4602-94e6-c645569a1f8b"},"outputs":[{"output_type":"stream","name":"stdout","text":["+--------+\n","|count(1)|\n","+--------+\n","|  239659|\n","+--------+\n","\n"]}],"source":["# Example 2\n","spark.sql(\"SELECT count(*) FROM utilization WHERE session_count > 70\").show()"]},{"cell_type":"code","execution_count":66,"metadata":{"id":"H0wLPSFWY1Wu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1732774713647,"user_tz":-480,"elapsed":1341,"user":{"displayName":"Nurul Izzati Darul Zaman","userId":"02130484108045505747"}},"outputId":"aac49458-3170-4711-a1aa-ee1e761f3be2"},"outputs":[{"output_type":"stream","name":"stdout","text":["+---------+--------+\n","|server_id|count(1)|\n","+---------+--------+\n","|      108|    8375|\n","|      101|    9808|\n","|      115|    5284|\n","|      126|    6365|\n","|      103|    8744|\n","|      128|    3719|\n","|      122|    4885|\n","|      111|    3093|\n","|      120|    2733|\n","|      117|    3605|\n","|      112|    7425|\n","|      127|    5961|\n","|      107|    5646|\n","|      114|    2128|\n","|      100|     391|\n","|      130|    2891|\n","|      129|    3222|\n","|      102|    8586|\n","|      113|    9418|\n","|      121|    7084|\n","+---------+--------+\n","only showing top 20 rows\n","\n"]}],"source":["# Example 3\n","spark.sql(\"SELECT server_id, count(*) FROM utilization \\\n","           WHERE session_count > 70 \\\n","           GROUP BY server_id\").show()"]},{"cell_type":"code","execution_count":67,"metadata":{"id":"aOTPk_4NY1Wu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1732774714566,"user_tz":-480,"elapsed":920,"user":{"displayName":"Nurul Izzati Darul Zaman","userId":"02130484108045505747"}},"outputId":"64be0a75-a143-4b8e-8809-2cfa9e176b4e"},"outputs":[{"output_type":"stream","name":"stdout","text":["+---------+--------+\n","|server_id|count(1)|\n","+---------+--------+\n","|      101|    9808|\n","|      113|    9418|\n","|      145|    9304|\n","|      103|    8744|\n","|      102|    8586|\n","|      133|    8583|\n","|      108|    8375|\n","|      149|    8288|\n","|      137|    8248|\n","|      148|    8027|\n","|      123|    7918|\n","|      118|    7913|\n","|      112|    7425|\n","|      139|    7383|\n","|      104|    7366|\n","|      121|    7084|\n","|      142|    7084|\n","|      146|    7072|\n","|      126|    6365|\n","|      144|    6220|\n","+---------+--------+\n","only showing top 20 rows\n","\n"]}],"source":["# Example 4\n","spark.sql(\"SELECT server_id, count(*) FROM utilization \\\n","           WHERE session_count > 70 \\\n","           GROUP BY server_id \\\n","           ORDER BY count(*) DESC\").show()"]},{"cell_type":"code","execution_count":68,"metadata":{"id":"w1u2IoSqY1Wu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1732774716313,"user_tz":-480,"elapsed":1750,"user":{"displayName":"Nurul Izzati Darul Zaman","userId":"02130484108045505747"}},"outputId":"38f6c463-c90a-47be-8caf-c0f419dd42d7"},"outputs":[{"output_type":"stream","name":"stdout","text":["+---------+------------------+------------------+----------------------------+\n","|server_id|min(session_count)|max(session_count)|round(avg(session_count), 2)|\n","+---------+------------------+------------------+----------------------------+\n","|      101|                71|               105|                       87.67|\n","|      113|                71|               103|                       86.96|\n","|      145|                71|               103|                       86.98|\n","|      103|                71|               101|                       85.76|\n","|      102|                71|               101|                       85.71|\n","|      133|                71|               100|                       85.47|\n","|      108|                71|               100|                       85.12|\n","|      149|                71|                99|                       84.96|\n","|      137|                71|                99|                       85.01|\n","|      148|                71|                99|                        84.7|\n","|      123|                71|                98|                       84.53|\n","|      118|                71|                98|                       84.66|\n","|      112|                71|                97|                       83.55|\n","|      139|                71|                96|                       83.33|\n","|      104|                71|                96|                       83.35|\n","|      121|                71|                95|                       82.89|\n","|      142|                71|                95|                        82.9|\n","|      146|                71|                95|                       82.95|\n","|      126|                71|                93|                       81.56|\n","|      144|                71|                92|                       81.38|\n","+---------+------------------+------------------+----------------------------+\n","only showing top 20 rows\n","\n"]}],"source":["# Example 5\n","spark.sql(\"SELECT server_id, min(session_count), max(session_count), \\\n","           round(avg(session_count),2) \\\n","           FROM utilization \\\n","           WHERE session_count > 70 \\\n","           GROUP BY server_id \\\n","           ORDER BY count(*) DESC\").show()"]},{"cell_type":"markdown","metadata":{"id":"SptPc7SWY1Wv"},"source":["### Joining DataFrames with SQL"]},{"cell_type":"markdown","metadata":{"id":"iKkfCsu9Y1Wv"},"source":["One of the most useful features of SQL is the ability to join tables. We can join in Spark SQL as well."]},{"cell_type":"markdown","metadata":{"id":"-I4VnSPaY1Ww"},"source":["First, we are going to create another temporary table based on the `server_names.csv` file."]},{"cell_type":"code","execution_count":70,"metadata":{"id":"CTDJgS6pY1Ww","executionInfo":{"status":"ok","timestamp":1732774726281,"user_tz":-480,"elapsed":366,"user":{"displayName":"Nurul Izzati Darul Zaman","userId":"02130484108045505747"}}},"outputs":[],"source":["file_path =MAIN_DIRECTORY+\"/data/server_names.csv\"\n","df_servers = spark.read.csv(file_path, header=True)"]},{"cell_type":"code","execution_count":71,"metadata":{"id":"j4X2d-LxY1Ww","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1732774730062,"user_tz":-480,"elapsed":328,"user":{"displayName":"Nurul Izzati Darul Zaman","userId":"02130484108045505747"}},"outputId":"34f91dee-588d-468c-df49-75cb530b6187"},"outputs":[{"output_type":"stream","name":"stdout","text":["+---------+-----------+\n","|server_id|server_name|\n","+---------+-----------+\n","|      100| Server 100|\n","|      101| Server 101|\n","|      102| Server 102|\n","|      103| Server 103|\n","|      104| Server 104|\n","|      105| Server 105|\n","|      106| Server 106|\n","|      107| Server 107|\n","|      108| Server 108|\n","|      109| Server 109|\n","|      110| Server 110|\n","|      111| Server 111|\n","|      112| Server 112|\n","|      113| Server 113|\n","|      114| Server 114|\n","|      115| Server 115|\n","|      116| Server 116|\n","|      117| Server 117|\n","|      118| Server 118|\n","|      119| Server 119|\n","+---------+-----------+\n","only showing top 20 rows\n","\n"]}],"source":["df_servers.show()"]},{"cell_type":"code","execution_count":72,"metadata":{"id":"F8hTyESfY1Wx","executionInfo":{"status":"ok","timestamp":1732774733213,"user_tz":-480,"elapsed":412,"user":{"displayName":"Nurul Izzati Darul Zaman","userId":"02130484108045505747"}}},"outputs":[],"source":["df_servers.createOrReplaceTempView('server_name')"]},{"cell_type":"markdown","metadata":{"id":"fWFNDVJ5Y1Wx"},"source":["Now, let's quickly do a check on `server_id` in `utilization` table."]},{"cell_type":"code","execution_count":73,"metadata":{"id":"JtNl9BnSY1Wx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1732774736691,"user_tz":-480,"elapsed":1848,"user":{"displayName":"Nurul Izzati Darul Zaman","userId":"02130484108045505747"}},"outputId":"097f1168-9ee7-48b7-d01c-7fe655bd1cf1"},"outputs":[{"output_type":"stream","name":"stdout","text":["+---------+\n","|server_id|\n","+---------+\n","|      100|\n","|      101|\n","|      102|\n","|      103|\n","|      104|\n","|      105|\n","|      106|\n","|      107|\n","|      108|\n","|      109|\n","|      110|\n","|      111|\n","|      112|\n","|      113|\n","|      114|\n","|      115|\n","|      116|\n","|      117|\n","|      118|\n","|      119|\n","+---------+\n","only showing top 20 rows\n","\n"]}],"source":["spark.sql('SELECT DISTINCT server_id FROM utilization ORDER BY server_id').show()"]},{"cell_type":"markdown","metadata":{"id":"HmE-slIeY1Wy"},"source":["Now, let's see what the minimum and maximum of server_id is."]},{"cell_type":"code","execution_count":74,"metadata":{"id":"K4gDasEiY1Wy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1732774737229,"user_tz":-480,"elapsed":540,"user":{"displayName":"Nurul Izzati Darul Zaman","userId":"02130484108045505747"}},"outputId":"41426687-da25-4674-c45f-415f28fbe168"},"outputs":[{"output_type":"stream","name":"stdout","text":["+--------------+--------------+\n","|min(server_id)|max(server_id)|\n","+--------------+--------------+\n","|           100|           149|\n","+--------------+--------------+\n","\n"]}],"source":["spark.sql(\"SELECT min(server_id), max(server_id) FROM utilization\").show()"]},{"cell_type":"markdown","metadata":{"id":"maWMLqguY1Wy"},"source":["Well, let's join these two tables."]},{"cell_type":"code","execution_count":75,"metadata":{"id":"uuxQMkypY1Wy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1732774737769,"user_tz":-480,"elapsed":541,"user":{"displayName":"Nurul Izzati Darul Zaman","userId":"02130484108045505747"}},"outputId":"be10ec1c-cb7c-469c-8413-a88fed4688e4"},"outputs":[{"output_type":"stream","name":"stdout","text":["+---------+-----------+-------------+\n","|server_id|server_name|session_count|\n","+---------+-----------+-------------+\n","|      100| Server 100|           47|\n","|      100| Server 100|           43|\n","|      100| Server 100|           62|\n","|      100| Server 100|           50|\n","|      100| Server 100|           43|\n","|      100| Server 100|           48|\n","|      100| Server 100|           58|\n","|      100| Server 100|           58|\n","|      100| Server 100|           62|\n","|      100| Server 100|           45|\n","|      100| Server 100|           47|\n","|      100| Server 100|           60|\n","|      100| Server 100|           57|\n","|      100| Server 100|           44|\n","|      100| Server 100|           47|\n","|      100| Server 100|           66|\n","|      100| Server 100|           65|\n","|      100| Server 100|           66|\n","|      100| Server 100|           42|\n","|      100| Server 100|           63|\n","+---------+-----------+-------------+\n","only showing top 20 rows\n","\n"]}],"source":["df_join = spark.sql(\"SELECT u.server_id, sn.server_name, u.session_count \\\n","                     FROM utilization u \\\n","                     INNER JOIN server_name sn \\\n","                     ON sn.server_id = u.server_id\")\n","df_join.show()"]},{"cell_type":"markdown","metadata":{"id":"DpnrpB2NY1Wz"},"source":["### De-Duplicating with DataFrame API"]},{"cell_type":"markdown","metadata":{"id":"jrHLa6eBY1Wz"},"source":["When we're working with Data Frames, Spark provides some ways to de-duplicate data. So, let's take a look at how to do that. In this part also we will learn how we can create small data sets to work within the Jupiter Notebook session. Before doing anything, please restart the Jupyter kernel."]},{"cell_type":"code","execution_count":76,"metadata":{"id":"ufoYqiESY1Wz","executionInfo":{"status":"ok","timestamp":1732775295727,"user_tz":-480,"elapsed":324,"user":{"displayName":"Nurul Izzati Darul Zaman","userId":"02130484108045505747"}}},"outputs":[],"source":["from pyspark import SparkContext\n","from pyspark.sql import SparkSession\n","from pyspark.sql import Row"]},{"cell_type":"code","execution_count":77,"metadata":{"id":"yPl4qX9FY1W0","executionInfo":{"status":"ok","timestamp":1732775296139,"user_tz":-480,"elapsed":1,"user":{"displayName":"Nurul Izzati Darul Zaman","userId":"02130484108045505747"}}},"outputs":[],"source":["sc =SparkContext.getOrCreate()"]},{"cell_type":"markdown","metadata":{"id":"XwDnJoJbY1W0"},"source":["`sc` stands for `SparkContext`. It is a global variable that gives us access to the Spark Context. Here, what we want to do is create a DataFrame, and to do that, we will use `parallelize` method that creates a parallelized data structure. Spark automatically parallelize DataFrames. But now we are going to create this data manually, so we are specifying `parallelized` explicitly."]},{"cell_type":"code","execution_count":78,"metadata":{"id":"JIRUvKslY1W0","executionInfo":{"status":"ok","timestamp":1732775298016,"user_tz":-480,"elapsed":395,"user":{"displayName":"Nurul Izzati Darul Zaman","userId":"02130484108045505747"}}},"outputs":[],"source":["rdd = sc.parallelize([Row(server_name='Server 101', cpu_utilization=85, session_count=80),\n","                      Row(server_name='Server 101', cpu_utilization=80, session_count=90),\n","                      Row(server_name='Server 102', cpu_utilization=85, session_count=80),\n","                      Row(server_name='Server 102', cpu_utilization=85, session_count=80)])"]},{"cell_type":"code","execution_count":79,"metadata":{"id":"_GTVK2NVY1W0","executionInfo":{"status":"ok","timestamp":1732775298430,"user_tz":-480,"elapsed":2,"user":{"displayName":"Nurul Izzati Darul Zaman","userId":"02130484108045505747"}}},"outputs":[],"source":["spark = SparkSession(sc)"]},{"cell_type":"markdown","metadata":{"id":"tMrSVgwqY1W0"},"source":["`toDF()` turns that parallelized data structure to into a DataFrame."]},{"cell_type":"code","execution_count":80,"metadata":{"id":"iWIJ1G2OY1W1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1732775661252,"user_tz":-480,"elapsed":2602,"user":{"displayName":"Nurul Izzati Darul Zaman","userId":"02130484108045505747"}},"outputId":"54c050b4-8cd6-4fc5-fbe3-0c50193bd376"},"outputs":[{"output_type":"stream","name":"stdout","text":["+-----------+---------------+-------------+\n","|server_name|cpu_utilization|session_count|\n","+-----------+---------------+-------------+\n","| Server 101|             85|           80|\n","| Server 101|             80|           90|\n","| Server 102|             85|           80|\n","| Server 102|             85|           80|\n","+-----------+---------------+-------------+\n","\n"]}],"source":["df_dup = rdd.toDF()\n","df_dup.show()"]},{"cell_type":"markdown","metadata":{"id":"M22LgcgdY1W1"},"source":["Now, we are going to drop duplicates. To do that we can use `drop_duplicates()` method which returns a new DataFrame with duplicate rows removed, optionally only considering certain columns."]},{"cell_type":"code","execution_count":81,"metadata":{"id":"YwYSHrhiY1W1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1732775661809,"user_tz":-480,"elapsed":559,"user":{"displayName":"Nurul Izzati Darul Zaman","userId":"02130484108045505747"}},"outputId":"242ca25c-53cb-4305-a17e-938d8c3d18a1"},"outputs":[{"output_type":"stream","name":"stdout","text":["+-----------+---------------+-------------+\n","|server_name|cpu_utilization|session_count|\n","+-----------+---------------+-------------+\n","| Server 101|             85|           80|\n","| Server 101|             80|           90|\n","| Server 102|             85|           80|\n","+-----------+---------------+-------------+\n","\n"]}],"source":["  df_dup.drop_duplicates().show()"]},{"cell_type":"markdown","metadata":{"id":"6OjLQK7wY1W1"},"source":["If we want to drop any time there is a duplicate in one of the columns, we can do that as well. Let's take a look at the following example."]},{"cell_type":"code","execution_count":82,"metadata":{"id":"01tASdurY1W1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1732775663028,"user_tz":-480,"elapsed":1221,"user":{"displayName":"Nurul Izzati Darul Zaman","userId":"02130484108045505747"}},"outputId":"86edbc35-be71-4f2e-9d42-2436806d6e40"},"outputs":[{"output_type":"stream","name":"stdout","text":["+-----------+---------------+-------------+\n","|server_name|cpu_utilization|session_count|\n","+-----------+---------------+-------------+\n","| Server 101|             85|           80|\n","| Server 102|             85|           80|\n","+-----------+---------------+-------------+\n","\n"]}],"source":[" df_dup.drop_duplicates(['server_name']).show()"]},{"cell_type":"code","execution_count":83,"metadata":{"id":"uUC_m-pGY1W2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1732775808395,"user_tz":-480,"elapsed":1369,"user":{"displayName":"Nurul Izzati Darul Zaman","userId":"02130484108045505747"}},"outputId":"99fea5c0-cbd9-4c58-bc24-829ab379752c"},"outputs":[{"output_type":"stream","name":"stdout","text":["+-----------+---------------+-------------+\n","|server_name|cpu_utilization|session_count|\n","+-----------+---------------+-------------+\n","| Server 101|             80|           90|\n","| Server 101|             85|           80|\n","| Server 102|             85|           80|\n","+-----------+---------------+-------------+\n","\n"]}],"source":[" df_dup.drop_duplicates(['server_name', 'cpu_utilization']).show()"]},{"cell_type":"markdown","metadata":{"id":"NxuhE2vPY1W2"},"source":["### Working with null values"]},{"cell_type":"markdown","metadata":{"id":"Vp8oU_ZfY1W2"},"source":["It is not uncommon to have data missing from DataFrame. When we are working with SQL, we are used to work with nulls. When we working with DataFrames, the absence of data is indicated by an NA. So in this section, we are going to look how we can work with NAs and Nulls using DataFrames and Spark SQL."]},{"cell_type":"markdown","metadata":{"id":"Yl81fu_KY1W2"},"source":["In this section, we are importing a couple of things, we have not seen before. Let's take a look at them."]},{"cell_type":"code","execution_count":84,"metadata":{"id":"ufkuUuK7Y1W2","executionInfo":{"status":"ok","timestamp":1732775808395,"user_tz":-480,"elapsed":2,"user":{"displayName":"Nurul Izzati Darul Zaman","userId":"02130484108045505747"}}},"outputs":[],"source":["from pyspark.sql.functions import lit #allows us to create a literal column for a dataframe\n","from pyspark.sql.types import StringType"]},{"cell_type":"markdown","metadata":{"id":"ZOmnOtzoY1W3"},"source":["Now, we are going to add a column and set that column's values equall to null or NA. In this case, we will use `lit()` function that is a way for us to interact with column literals in PySpark. Spark SQL functions lit() is used to add a new column by assigning a literal or constant value to Spark DataFrame."]},{"cell_type":"code","execution_count":85,"metadata":{"id":"ub-l-9P3Y1W3","executionInfo":{"status":"ok","timestamp":1732775808396,"user_tz":-480,"elapsed":3,"user":{"displayName":"Nurul Izzati Darul Zaman","userId":"02130484108045505747"}}},"outputs":[],"source":["df = rdd.toDF()\n","df_na = df.withColumn('na_col', lit(None).cast(StringType()))"]},{"cell_type":"code","execution_count":86,"metadata":{"id":"0CPQ_vQEY1W3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1732775809737,"user_tz":-480,"elapsed":1344,"user":{"displayName":"Nurul Izzati Darul Zaman","userId":"02130484108045505747"}},"outputId":"4d51cdcc-b777-466c-fafb-7ae42a6b2d85"},"outputs":[{"output_type":"stream","name":"stdout","text":["+-----------+---------------+-------------+------+\n","|server_name|cpu_utilization|session_count|na_col|\n","+-----------+---------------+-------------+------+\n","| Server 101|             85|           80|  NULL|\n","| Server 101|             80|           90|  NULL|\n","| Server 102|             85|           80|  NULL|\n","| Server 102|             85|           80|  NULL|\n","+-----------+---------------+-------------+------+\n","\n"]}],"source":["df_na.show()"]},{"cell_type":"markdown","metadata":{"id":"D8UPva3bY1W4"},"source":["Now, one of the things that we can do is globally replace all nulls or NAs with some value. And we can do that with `fillna()` function."]},{"cell_type":"code","execution_count":87,"metadata":{"id":"Dz1h3QtZY1W4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1732775809737,"user_tz":-480,"elapsed":7,"user":{"displayName":"Nurul Izzati Darul Zaman","userId":"02130484108045505747"}},"outputId":"fb6de2c9-0786-4b2b-f577-411bceff7787"},"outputs":[{"output_type":"stream","name":"stdout","text":["+-----------+---------------+-------------+------+\n","|server_name|cpu_utilization|session_count|na_col|\n","+-----------+---------------+-------------+------+\n","| Server 101|             85|           80|     A|\n","| Server 101|             80|           90|     A|\n","| Server 102|             85|           80|     A|\n","| Server 102|             85|           80|     A|\n","+-----------+---------------+-------------+------+\n","\n"]}],"source":["df_na.fillna('A').show()"]},{"cell_type":"markdown","metadata":{"id":"XDCgVwQkY1W4"},"source":["Now, Let's create a DataFrame that has versions both with the nulls and with the As."]},{"cell_type":"code","execution_count":88,"metadata":{"id":"h0vPjDA3Y1W4","executionInfo":{"status":"ok","timestamp":1732775898201,"user_tz":-480,"elapsed":1331,"user":{"displayName":"Nurul Izzati Darul Zaman","userId":"02130484108045505747"}}},"outputs":[],"source":["df_union = df_na.fillna('A').union(df_na)"]},{"cell_type":"code","execution_count":89,"metadata":{"id":"VStlK4bKY1W4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1732775901164,"user_tz":-480,"elapsed":2964,"user":{"displayName":"Nurul Izzati Darul Zaman","userId":"02130484108045505747"}},"outputId":"c2411b09-01ae-4ea1-cbcb-c30bc00c3b59"},"outputs":[{"output_type":"stream","name":"stdout","text":["+-----------+---------------+-------------+------+\n","|server_name|cpu_utilization|session_count|na_col|\n","+-----------+---------------+-------------+------+\n","| Server 101|             85|           80|     A|\n","| Server 101|             80|           90|     A|\n","| Server 102|             85|           80|     A|\n","| Server 102|             85|           80|     A|\n","| Server 101|             85|           80|  NULL|\n","| Server 101|             80|           90|  NULL|\n","| Server 102|             85|           80|  NULL|\n","| Server 102|             85|           80|  NULL|\n","+-----------+---------------+-------------+------+\n","\n"]}],"source":["df_union.show()"]},{"cell_type":"markdown","metadata":{"id":"-AvtNqhIY1W5"},"source":["Now we can drop only rows with NAs in them."]},{"cell_type":"code","execution_count":90,"metadata":{"id":"y0LlSjvYY1W5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1732775901466,"user_tz":-480,"elapsed":305,"user":{"displayName":"Nurul Izzati Darul Zaman","userId":"02130484108045505747"}},"outputId":"1af04f73-1903-4373-cef7-92bdcfc1f872"},"outputs":[{"output_type":"stream","name":"stdout","text":["+-----------+---------------+-------------+------+\n","|server_name|cpu_utilization|session_count|na_col|\n","+-----------+---------------+-------------+------+\n","| Server 101|             85|           80|     A|\n","| Server 101|             80|           90|     A|\n","| Server 102|             85|           80|     A|\n","| Server 102|             85|           80|     A|\n","+-----------+---------------+-------------+------+\n","\n"]}],"source":["df_union.na.drop().show()"]},{"cell_type":"markdown","metadata":{"id":"ZY695s0FY1W5"},"source":["Well, let's see how we can do that with Spark SQL."]},{"cell_type":"code","execution_count":92,"metadata":{"id":"RUqubWJDY1W5","executionInfo":{"status":"ok","timestamp":1732775905173,"user_tz":-480,"elapsed":1,"user":{"displayName":"Nurul Izzati Darul Zaman","userId":"02130484108045505747"}}},"outputs":[],"source":["df_union.createOrReplaceTempView('na_table')"]},{"cell_type":"code","execution_count":93,"metadata":{"id":"fI9y-weJY1W6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1732775906754,"user_tz":-480,"elapsed":920,"user":{"displayName":"Nurul Izzati Darul Zaman","userId":"02130484108045505747"}},"outputId":"5b2fe299-cc73-4d10-a0a2-0dcac81eac37"},"outputs":[{"output_type":"stream","name":"stdout","text":["+-----------+---------------+-------------+------+\n","|server_name|cpu_utilization|session_count|na_col|\n","+-----------+---------------+-------------+------+\n","| Server 101|             85|           80|  NULL|\n","| Server 101|             80|           90|  NULL|\n","| Server 102|             85|           80|  NULL|\n","| Server 102|             85|           80|  NULL|\n","+-----------+---------------+-------------+------+\n","\n"]}],"source":["spark.sql('SELECT * FROM na_table WHERE na_col IS NULL').show()"]},{"cell_type":"markdown","metadata":{"id":"3y78PcUVY1W6"},"source":["## Exploratory Data Analysis with DataFrame API"]},{"cell_type":"markdown","metadata":{"id":"Yxj1hDozY1W6"},"source":["DataFrame API provides some tools for some higher level tasks like exploratory data analysis. In this section, we are going to learn how to use DataFrame API for doing some basic EDA with the utilization DataFrame. First, let's take a look at this DataFrame."]},{"cell_type":"code","execution_count":94,"metadata":{"id":"kLDFQ4r4Y1W6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1732775908080,"user_tz":-480,"elapsed":2,"user":{"displayName":"Nurul Izzati Darul Zaman","userId":"02130484108045505747"}},"outputId":"9acac9e3-5db6-4045-f5aa-862ecd715f9c"},"outputs":[{"output_type":"stream","name":"stdout","text":["+-------------------+---------+---------------+-----------+-------------+\n","|     event_datetime|server_id|cpu_utilization|free_memory|session_count|\n","+-------------------+---------+---------------+-----------+-------------+\n","|03/05/2019 08:06:14|      100|           0.57|       0.51|           47|\n","|03/05/2019 08:11:14|      100|           0.47|       0.62|           43|\n","|03/05/2019 08:16:14|      100|           0.56|       0.57|           62|\n","|03/05/2019 08:21:14|      100|           0.57|       0.56|           50|\n","|03/05/2019 08:26:14|      100|           0.35|       0.46|           43|\n","|03/05/2019 08:31:14|      100|           0.41|       0.58|           48|\n","|03/05/2019 08:36:14|      100|           0.57|       0.35|           58|\n","|03/05/2019 08:41:14|      100|           0.41|        0.4|           58|\n","|03/05/2019 08:46:14|      100|           0.53|       0.35|           62|\n","|03/05/2019 08:51:14|      100|           0.51|        0.6|           45|\n","|03/05/2019 08:56:14|      100|           0.32|       0.37|           47|\n","|03/05/2019 09:01:14|      100|           0.62|       0.59|           60|\n","|03/05/2019 09:06:14|      100|           0.66|       0.72|           57|\n","|03/05/2019 09:11:14|      100|           0.54|       0.54|           44|\n","|03/05/2019 09:16:14|      100|           0.29|        0.4|           47|\n","|03/05/2019 09:21:14|      100|           0.43|       0.68|           66|\n","|03/05/2019 09:26:14|      100|           0.49|       0.66|           65|\n","|03/05/2019 09:31:14|      100|           0.64|       0.55|           66|\n","|03/05/2019 09:36:14|      100|           0.42|        0.6|           42|\n","|03/05/2019 09:41:14|      100|           0.55|       0.59|           63|\n","+-------------------+---------+---------------+-----------+-------------+\n","only showing top 20 rows\n","\n"]}],"source":["df_util.show()"]},{"cell_type":"code","execution_count":95,"metadata":{"id":"LtWQEjEVY1W7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1732775908864,"user_tz":-480,"elapsed":2,"user":{"displayName":"Nurul Izzati Darul Zaman","userId":"02130484108045505747"}},"outputId":"bebcfa1c-822d-4fc9-9ada-3b8a43f90624"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["500000"]},"metadata":{},"execution_count":95}],"source":["df_util.count()"]},{"cell_type":"markdown","metadata":{"id":"m8AtjcssY1W7"},"source":["One of the useful methods for doing exploratory data analysis is `.describe()`. Let's see how it works."]},{"cell_type":"code","execution_count":96,"metadata":{"id":"sx09zri_Y1W7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1732775924579,"user_tz":-480,"elapsed":6057,"user":{"displayName":"Nurul Izzati Darul Zaman","userId":"02130484108045505747"}},"outputId":"a0ade618-e228-4537-af65-afe86b97bcbc"},"outputs":[{"output_type":"stream","name":"stdout","text":["+-------+-------------------+------------------+-------------------+-------------------+------------------+\n","|summary|     event_datetime|         server_id|    cpu_utilization|        free_memory|     session_count|\n","+-------+-------------------+------------------+-------------------+-------------------+------------------+\n","|  count|             500000|            500000|             500000|             500000|            500000|\n","|   mean|               NULL|             124.5| 0.6205177399999797|0.37912809999999125|          69.59616|\n","| stddev|               NULL|14.430884120552715|0.15875173872913106|0.15830931278376184|14.850676696352798|\n","|    min|03/05/2019 08:06:14|               100|               0.22|                0.0|                32|\n","|    max|04/09/2019 01:22:46|               149|                1.0|               0.78|               105|\n","+-------+-------------------+------------------+-------------------+-------------------+------------------+\n","\n"]}],"source":["df_util.describe().show()"]},{"cell_type":"markdown","metadata":{"id":"JdlMAie4Y1W7"},"source":["`.describe()` actually produces another DataFrame with summary statistics about the DataFrame. For example, in this case, we see that there are several columns; there is a summary column, followed by the name of a column in the original DataFrame. For each of those columns in the original DataFrame, we have the same statistics that are calculated.\n","Using `.describe()`  is an excellent way to get a high-level view of what a data set might be like.\n","\n","Another statistics we often want to know, is there a correlation between two of the variables?"]},{"cell_type":"code","execution_count":97,"metadata":{"id":"PHVyHxeXY1W8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1732776140171,"user_tz":-480,"elapsed":355,"user":{"displayName":"Nurul Izzati Darul Zaman","userId":"02130484108045505747"}},"outputId":"157f0c25-b870-489e-ffc5-d38f0b9d925d"},"outputs":[{"output_type":"stream","name":"stdout","text":["root\n"," |-- event_datetime: string (nullable = true)\n"," |-- server_id: integer (nullable = true)\n"," |-- cpu_utilization: double (nullable = true)\n"," |-- free_memory: double (nullable = true)\n"," |-- session_count: integer (nullable = true)\n","\n"]}],"source":["df_util.printSchema()"]},{"cell_type":"code","execution_count":98,"metadata":{"id":"xgVBps1aY1W8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1732776142367,"user_tz":-480,"elapsed":1853,"user":{"displayName":"Nurul Izzati Darul Zaman","userId":"02130484108045505747"}},"outputId":"3af60eab-5f51-4348-f367-e77e121eca75"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["-0.47047715730806805"]},"metadata":{},"execution_count":98}],"source":["df_util.stat.corr('cpu_utilization', 'free_memory')"]},{"cell_type":"code","execution_count":99,"metadata":{"id":"Tk5E3T8UY1W8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1732776146964,"user_tz":-480,"elapsed":1675,"user":{"displayName":"Nurul Izzati Darul Zaman","userId":"02130484108045505747"}},"outputId":"4ec29765-d1e6-43d3-facf-0fd689639dbf"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["-0.5008320848876696"]},"metadata":{},"execution_count":99}],"source":["df_util.stat.corr('session_count','free_memory')"]},{"cell_type":"markdown","metadata":{"id":"lTElR0ZfY1W9"},"source":["Sometimes we want to know how frequent are some items, what are the most frequently occurring items?\n","\n","There is a method called `.freq()` items for frequent items, which we can use with the DataFrame."]},{"cell_type":"code","execution_count":100,"metadata":{"id":"u6CMbdTlY1W9","executionInfo":{"status":"ok","timestamp":1732776149561,"user_tz":-480,"elapsed":507,"user":{"displayName":"Nurul Izzati Darul Zaman","userId":"02130484108045505747"}}},"outputs":[],"source":["df_freqItems = df_util.stat.freqItems(['server_id'])"]},{"cell_type":"code","execution_count":101,"metadata":{"scrolled":true,"id":"rz2A_r1uY1W9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1732776151705,"user_tz":-480,"elapsed":1829,"user":{"displayName":"Nurul Izzati Darul Zaman","userId":"02130484108045505747"}},"outputId":"ecb88f26-6b21-48a2-a03b-88549a25f774"},"outputs":[{"output_type":"stream","name":"stdout","text":["+--------------------+-----------------------+\n","| server_id_freqItems|session_count_freqItems|\n","+--------------------+-----------------------+\n","|[146, 137, 101, 1...|   [92, 101, 83, 104...|\n","+--------------------+-----------------------+\n","\n"]}],"source":["df_util.stat.freqItems(['server_id','session_count']).show()"]},{"cell_type":"markdown","metadata":{"id":"podjmTTOY1W-"},"source":["We can create a result-set that shows some basic statistics for one of the columns by using Spark SQL. Let's do it."]},{"cell_type":"code","execution_count":102,"metadata":{"id":"zOPQoHaeY1W-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1732776152761,"user_tz":-480,"elapsed":1058,"user":{"displayName":"Nurul Izzati Darul Zaman","userId":"02130484108045505747"}},"outputId":"d5c7c2e4-84be-446e-fff1-062af66e0f39"},"outputs":[{"output_type":"stream","name":"stdout","text":["+--------------------+--------------------+-----------------------+\n","|min(cpu_utilization)|max(cpu_utilization)|stddev(cpu_utilization)|\n","+--------------------+--------------------+-----------------------+\n","|                0.22|                 1.0|    0.15875173872913106|\n","+--------------------+--------------------+-----------------------+\n","\n"]}],"source":["spark.sql(\"SELECT min(cpu_utilization),\\\n","            max(cpu_utilization),stddev(cpu_utilization) FROM utilization\").show()"]},{"cell_type":"markdown","metadata":{"id":"LVxZ6JM_Y1W-"},"source":["And if we want to group the result-set by `server_id`, we can write the following query."]},{"cell_type":"code","execution_count":103,"metadata":{"id":"5zGupeY-Y1W-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1732776156199,"user_tz":-480,"elapsed":1614,"user":{"displayName":"Nurul Izzati Darul Zaman","userId":"02130484108045505747"}},"outputId":"5107db1f-02de-499c-a596-353d60a3a64f"},"outputs":[{"output_type":"stream","name":"stdout","text":["+---------+--------------------+--------------------+-----------------------+\n","|server_id|min(cpu_utilization)|max(cpu_utilization)|stddev(cpu_utilization)|\n","+---------+--------------------+--------------------+-----------------------+\n","|      108|                0.55|                0.95|    0.11563100171171926|\n","|      101|                 0.6|                 1.0|    0.11651726263197697|\n","|      115|                0.44|                0.84|    0.11569664615014985|\n","|      126|                0.48|                0.88|    0.11542612970702051|\n","|      103|                0.56|                0.96|    0.11617507884178278|\n","|      128|                0.38|                0.78|     0.1153254132405078|\n","|      122|                0.43|                0.83|    0.11563104329209034|\n","|      111|                0.36|                0.76|    0.11530221569464506|\n","|      120|                0.35|                0.75|    0.11586355920838642|\n","|      117|                0.38|                0.78|    0.11534593941519553|\n","|      112|                0.52|                0.92|    0.11528867845082576|\n","|      127|                0.47|                0.87|    0.11577746913037888|\n","|      107|                0.45|                0.85|    0.11597417369783877|\n","|      114|                0.33|                0.73|    0.11510268816097273|\n","|      100|                0.27|                0.67|     0.1152264191787964|\n","|      130|                0.35|                0.75|    0.11568834774246008|\n","|      129|                0.37|                0.77|    0.11644328829064748|\n","|      102|                0.56|                0.96|    0.11549678751286807|\n","|      113|                0.58|                0.98|    0.11544345150353694|\n","|      121|                 0.5|                 0.9|    0.11441714332768797|\n","+---------+--------------------+--------------------+-----------------------+\n","only showing top 20 rows\n","\n"]}],"source":["spark.sql(\"SELECT server_id, min(cpu_utilization),max(cpu_utilization),stddev(cpu_utilization) \\\n","           FROM utilization \\\n","           GROUP BY server_id\").show()"]},{"cell_type":"markdown","metadata":{"id":"WpWoMmS8Y1W_"},"source":["Now, we are going to calculate statistics on buckets or histograms of data. The idea is, rather than look at each server individually, Spark buckets values according to how frequently they occur in certain ranges. So if we want to know how often does a CPU utilization fall in the range of 1-10 or 11-20 or 21-30, all the way up to 90-91, we could put each of those CPU utilization measures into its bucket and count how many times a CPU utilization goes into that bucket. Let's do it."]},{"cell_type":"code","execution_count":104,"metadata":{"id":"XdCEtkmSY1W_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1732776218302,"user_tz":-480,"elapsed":340,"user":{"displayName":"Nurul Izzati Darul Zaman","userId":"02130484108045505747"}},"outputId":"2baf130e-0b4d-464e-f489-93e1c4f90ebe"},"outputs":[{"output_type":"stream","name":"stdout","text":["+---------+------+\n","|server_id|Bucket|\n","+---------+------+\n","|      100|     5|\n","|      100|     4|\n","|      100|     5|\n","|      100|     5|\n","|      100|     3|\n","|      100|     4|\n","|      100|     5|\n","|      100|     4|\n","|      100|     5|\n","|      100|     5|\n","|      100|     3|\n","|      100|     6|\n","|      100|     6|\n","|      100|     5|\n","|      100|     2|\n","|      100|     4|\n","|      100|     4|\n","|      100|     6|\n","|      100|     4|\n","|      100|     5|\n","+---------+------+\n","only showing top 20 rows\n","\n"]}],"source":["spark.sql(\"SELECT server_id, FLOOR(cpu_utilization*100/10) as Bucket FROM utilization\").show()"]},{"cell_type":"markdown","metadata":{"id":"nfES_UNxY1W_"},"source":["So far, what we have done is we have listed for each server in what  CPU utilization bucket falls at a particular time. Now we want to see how often does a CPU utilization falls into one of those ten buckets."]},{"cell_type":"code","execution_count":105,"metadata":{"scrolled":true,"id":"8JwEARe1Y1W_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1732776220454,"user_tz":-480,"elapsed":1763,"user":{"displayName":"Nurul Izzati Darul Zaman","userId":"02130484108045505747"}},"outputId":"4b4ae8a8-db5a-420f-970f-b5285aaf008b"},"outputs":[{"output_type":"stream","name":"stdout","text":["+--------+------+\n","|count(1)|Bucket|\n","+--------+------+\n","|    8186|     2|\n","|   37029|     3|\n","|   68046|     4|\n","|  104910|     5|\n","|  116725|     6|\n","|   88242|     7|\n","|   56598|     8|\n","|   20207|     9|\n","|      57|    10|\n","+--------+------+\n","\n"]}],"source":["spark.sql(\"SELECT count(*), FLOOR(cpu_utilization*100/10) as Bucket \\\n","           FROM utilization GROUP BY Bucket ORDER BY Bucket\").show()"]},{"cell_type":"markdown","metadata":{"id":"zWzX8mcRY1W_"},"source":["## Timeseries Analysis"]},{"cell_type":"markdown","metadata":{"id":"7ypicxzXY1XA"},"source":["In this section, we are going to work with timeseries data, and timeseries data has a set of measures and a timestamp associated with them. First, let's take a look at utilization table again."]},{"cell_type":"code","execution_count":106,"metadata":{"id":"P-0x9LYGY1XA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1732776224045,"user_tz":-480,"elapsed":1866,"user":{"displayName":"Nurul Izzati Darul Zaman","userId":"02130484108045505747"}},"outputId":"f3f8f32f-7011-4754-a129-52feee787353"},"outputs":[{"output_type":"stream","name":"stdout","text":["+---------+--------------------+--------------------+-----------------------+\n","|server_id|min(cpu_utilization)|max(cpu_utilization)|stddev(cpu_utilization)|\n","+---------+--------------------+--------------------+-----------------------+\n","|      108|                0.55|                0.95|    0.11563100171171926|\n","|      101|                 0.6|                 1.0|    0.11651726263197697|\n","|      115|                0.44|                0.84|    0.11569664615014985|\n","|      126|                0.48|                0.88|    0.11542612970702051|\n","|      103|                0.56|                0.96|    0.11617507884178278|\n","|      128|                0.38|                0.78|     0.1153254132405078|\n","|      122|                0.43|                0.83|    0.11563104329209034|\n","|      111|                0.36|                0.76|    0.11530221569464506|\n","|      120|                0.35|                0.75|    0.11586355920838642|\n","|      117|                0.38|                0.78|    0.11534593941519553|\n","|      112|                0.52|                0.92|    0.11528867845082576|\n","|      127|                0.47|                0.87|    0.11577746913037888|\n","|      107|                0.45|                0.85|    0.11597417369783877|\n","|      114|                0.33|                0.73|    0.11510268816097273|\n","|      100|                0.27|                0.67|     0.1152264191787964|\n","|      130|                0.35|                0.75|    0.11568834774246008|\n","|      129|                0.37|                0.77|    0.11644328829064748|\n","|      102|                0.56|                0.96|    0.11549678751286807|\n","|      113|                0.58|                0.98|    0.11544345150353694|\n","|      121|                 0.5|                 0.9|    0.11441714332768797|\n","+---------+--------------------+--------------------+-----------------------+\n","only showing top 20 rows\n","\n"]}],"source":["spark.sql(\"SELECT server_id, min(cpu_utilization), max(cpu_utilization), stddev(cpu_utilization) \\\n","           FROM utilization \\\n","           GROUP BY server_id\").show()"]},{"cell_type":"markdown","metadata":{"id":"X_ytBMk5Y1XA"},"source":["Sometimes we might want to compare a value within a group. For example, we would like to compare the current CPU utilization for a server to the average CPU utilization of just that server, not the entire population.\n","\n","We can do that using a windowing function, and in SQL, the windowing functions are specified using an `OVER...PARTITION BY` statement. So let's take a look at how to use that."]},{"cell_type":"code","execution_count":107,"metadata":{"id":"1aCXk1d7Y1XA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1732776226946,"user_tz":-480,"elapsed":2902,"user":{"displayName":"Nurul Izzati Darul Zaman","userId":"02130484108045505747"}},"outputId":"1cc1fe37-5d4b-4a6d-f3e6-3bbad99d523b"},"outputs":[{"output_type":"stream","name":"stdout","text":["+-------------------+---------+---------------+-----------------+\n","|     event_datetime|server_id|cpu_utilization|  avg_server_util|\n","+-------------------+---------+---------------+-----------------+\n","|03/05/2019 08:06:14|      100|           0.57|0.467506000000003|\n","|03/05/2019 08:11:14|      100|           0.47|0.467506000000003|\n","|03/05/2019 08:16:14|      100|           0.56|0.467506000000003|\n","|03/05/2019 08:21:14|      100|           0.57|0.467506000000003|\n","|03/05/2019 08:26:14|      100|           0.35|0.467506000000003|\n","|03/05/2019 08:31:14|      100|           0.41|0.467506000000003|\n","|03/05/2019 08:36:14|      100|           0.57|0.467506000000003|\n","|03/05/2019 08:41:14|      100|           0.41|0.467506000000003|\n","|03/05/2019 08:46:14|      100|           0.53|0.467506000000003|\n","|03/05/2019 08:51:14|      100|           0.51|0.467506000000003|\n","|03/05/2019 08:56:14|      100|           0.32|0.467506000000003|\n","|03/05/2019 09:01:14|      100|           0.62|0.467506000000003|\n","|03/05/2019 09:06:14|      100|           0.66|0.467506000000003|\n","|03/05/2019 09:11:14|      100|           0.54|0.467506000000003|\n","|03/05/2019 09:16:14|      100|           0.29|0.467506000000003|\n","|03/05/2019 09:21:14|      100|           0.43|0.467506000000003|\n","|03/05/2019 09:26:14|      100|           0.49|0.467506000000003|\n","|03/05/2019 09:31:14|      100|           0.64|0.467506000000003|\n","|03/05/2019 09:36:14|      100|           0.42|0.467506000000003|\n","|03/05/2019 09:41:14|      100|           0.55|0.467506000000003|\n","+-------------------+---------+---------------+-----------------+\n","only showing top 20 rows\n","\n"]}],"source":["spark.sql('SELECT event_datetime, server_id, cpu_utilization, \\\n","           avg(cpu_utilization) OVER (PARTITION BY server_id) as avg_server_util \\\n","           FROM Utilization').show()"]},{"cell_type":"markdown","metadata":{"id":"3rXevRZlY1XB"},"source":["Now, we have different timestamps for each server ID, different CPU utilization at those particular times, but in this piece of result-set, the average server utilization is always 0.7153 for server ID 112.\n","\n","Now, we want to calculate the difference any one of these measurements of CPU utilization from the average of that server is?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jHFpkZOGY1XB"},"outputs":[],"source":["spark.sql('SELECT event_datetime, server_id, cpu_utilization, \\\n","           avg(cpu_utilization) OVER (PARTITION BY server_id) as avg_server_util, \\\n","           cpu_utilization - avg(cpu_utilization) OVER (PARTITION BY server_id) as delta_server_util \\\n","           FROM Utilization').show()"]},{"cell_type":"markdown","metadata":{"id":"31zpnl7hY1XB"},"source":["That is one of the operations that we can do with the windowing functions. We can compare a particular value in a row to a value of some aggregate function applied to a sub-set of rows."]},{"cell_type":"markdown","metadata":{"id":"fqn8d1NmY1XC"},"source":["Another operation that we can do with windowing functions is looking around the neighborhood of a row. For example, we might want to calculate in a sliding window, look at the last three values and average them or look at the previous value, current value, next value, and average them. Let's do it."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7OgxOCmAY1XC"},"outputs":[],"source":["spark.sql('SELECT event_datetime, server_id, cpu_utilization, \\\n","           avg(cpu_utilization) OVER (PARTITION BY server_id ORDER BY event_datetime \\\n","                                       ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING) as avg_server_util \\\n","           FROM Utilization').show()"]},{"cell_type":"markdown","metadata":{"id":"x86_3DhjY1XC"},"source":["#### Great Job!"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}